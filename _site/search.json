[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Prisha Narasimhan",
    "section": "",
    "text": "Finley Malloc is the Chief Data Scientist at Wengo Analytics. When not innovating on data platforms, Finley enjoys spending time unicycling and playing with her pet iguana."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Prisha Narasimhan",
    "section": "Education",
    "text": "Education\nUniversity of California, Los Angeles | Los Angeles, CA  BAS in Statistics and Data Science and Economics  Minor in Global Studies | Sept 2022 - March 2026"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Prisha Narasimhan",
    "section": "Experience",
    "text": "Experience\nDeloitte Consulting | Intern | June 2025 - September 2025 \nR2 Global Technology Services | Business and Technology Intern | June 2024 - September 2024 \nUS Department of Energy: Loans Program Office | Intern | December 2023 - March 2024 \nSam Technology | Operations Intern | March 2023 - May 2023"
  },
  {
    "objectID": "projects/05-project.html",
    "href": "projects/05-project.html",
    "title": "Economics 103 - Gym Members Exercise Tracking",
    "section": "",
    "text": "library(readr)\ngym_members_exercise_tracking &lt;- read_csv(\"~/Econ 103/gym_members_exercise_tracking.csv\")\n\nRows: 973 Columns: 15\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): Gender, Workout_Type\ndbl (13): Age, Weight (kg), Height (m), Max_BPM, Avg_BPM, Resting_BPM, Sessi...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata = gym_members_exercise_tracking\n\n\nModel Building\nBasic Multiple Regression Model\n\nmodel &lt;- lm(`Weight (kg)` ~ BMI + `Height (m)` + Fat_Percentage + \n              `Water_Intake (liters)` + `Session_Duration (hours)` + Gender + \n              Workout_Type, \n            data = gym_members_exercise_tracking)\nplot(model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest for Transformations\n\ngym_members_exercise_tracking$log_Weight &lt;- \n  log(gym_members_exercise_tracking$`Weight (kg)`)\ngym_members_exercise_tracking$log_BMI &lt;- log(gym_members_exercise_tracking$BMI)\ngym_members_exercise_tracking$log_Height &lt;- \n  log(gym_members_exercise_tracking$`Height (m)`)\n\nmodel_log &lt;- lm(log_Weight ~ log_BMI + log_Height + Fat_Percentage + \n                  `Water_Intake (liters)` + `Session_Duration (hours)` + Gender \n                + Workout_Type, \n                data = gym_members_exercise_tracking)\nsummary(model_log)\n\n\nCall:\nlm(formula = log_Weight ~ log_BMI + log_Height + Fat_Percentage + \n    `Water_Intake (liters)` + `Session_Duration (hours)` + Gender + \n    Workout_Type, data = gym_members_exercise_tracking)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-3.820e-04 -9.716e-05  5.880e-06  9.825e-05  4.012e-04 \n\nCoefficients:\n                             Estimate Std. Error   t value Pr(&gt;|t|)    \n(Intercept)                -3.866e-05  9.642e-05    -0.401   0.6885    \nlog_BMI                     1.000e+00  1.845e-05 54203.765   &lt;2e-16 ***\nlog_Height                  2.000e+00  7.789e-05 25676.470   &lt;2e-16 ***\nFat_Percentage             -1.021e-07  1.002e-06    -0.102   0.9188    \n`Water_Intake (liters)`     9.701e-06  1.067e-05     0.909   0.3633    \n`Session_Duration (hours)` -2.476e-06  1.606e-05    -0.154   0.8775    \nGenderMale                 -2.850e-05  1.448e-05    -1.969   0.0493 *  \nWorkout_TypeHIIT           -5.324e-06  1.199e-05    -0.444   0.6570    \nWorkout_TypeStrength        3.372e-06  1.155e-05     0.292   0.7703    \nWorkout_TypeYoga            3.388e-06  1.176e-05     0.288   0.7734    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0001301 on 963 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:      1 \nF-statistic: 4.924e+08 on 9 and 963 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nApply the 3 log transformations that we found necessary earlier.\nCandidate Model\n\ncan_model &lt;- lm(log_Weight ~ log_BMI + log_Height + Fat_Percentage + \n                  `Water_Intake (liters)` + `Session_Duration (hours)` + Gender \n                + Workout_Type, \n                data = gym_members_exercise_tracking)\ncan_model\n\n\nCall:\nlm(formula = log_Weight ~ log_BMI + log_Height + Fat_Percentage + \n    `Water_Intake (liters)` + `Session_Duration (hours)` + Gender + \n    Workout_Type, data = gym_members_exercise_tracking)\n\nCoefficients:\n               (Intercept)                     log_BMI  \n                -3.866e-05                   1.000e+00  \n                log_Height              Fat_Percentage  \n                 2.000e+00                  -1.021e-07  \n   `Water_Intake (liters)`  `Session_Duration (hours)`  \n                 9.701e-06                  -2.476e-06  \n                GenderMale            Workout_TypeHIIT  \n                -2.850e-05                  -5.324e-06  \n      Workout_TypeStrength            Workout_TypeYoga  \n                 3.372e-06                   3.388e-06  \n\n\nBased on the tests run so far, the candidate model accounts for the necessary transformations required on the response and the predictors. With this candidate model, we are trying to find if there is a positive linear relationship between the response variable of weight and the 7 predictors (BMI, height, fat percentage, water intake, session duraction, gender, and workout type). This model is ideally able to take in the appropriately transformed variables and find if in fact there is such positive linear relationship as well as investigate other questions such as if there is multicollinearity between the factor variables and if certain predictors affect weight more than others. The above model is therefore the ideal candidate model to answer such questions.\n\n\nResidual Plot Transformed Model\n\nplot(model_log, which = 1)\n\n\n\n\n\n\n\n\nResidual Plot is randomly scattered around 0, indicating a good plot. Compared to the untransformed data, the residuals of the candidate model appear much more evenly distributed around the horizontal line at 0. However, the only area of concern is that there appears to be a slight narrowing of the residuals as the fitted values increase, hinting that there may be larger variances for smaller values.\n\n\nQQ Plot\n\nlibrary(car)\n\nLoading required package: carData\n\nqqPlot(model_log)\n\n\n\n\n\n\n\n\n[1] 868 909\n\noutlierTest(model_log)\n\nNo Studentized residuals with Bonferroni p &lt; 0.05\nLargest |rstudent|:\n    rstudent unadjusted p-value Bonferroni p\n909 3.126248          0.0018237           NA\n\n\nQQ Plot follows a straight line in general, indicating a normal distribution. However, 868 and 909 appear to be outliers. However, checking with a Bonferroni outlier test shows that these are not significant outliers.\n\n\nJarque Bera Test\n\nlibrary(tseries)\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n# Jarque Bera Test for Untransformed Model\njarque.bera.test(model$residuals)\n\n\n    Jarque Bera Test\n\ndata:  model$residuals\nX-squared = 187.81, df = 2, p-value &lt; 2.2e-16\n\n# Jarque Bera Test for Candidate Model\njarque.bera.test(model_log$residuals) \n\n\n    Jarque Bera Test\n\ndata:  model_log$residuals\nX-squared = 10.91, df = 2, p-value = 0.004274\n\n\nUsing the residual plot for the candidate model, the qq-plot, and the Jarque Bera test, we see that there is no need for further transformations on the candidate model. While the Jarque Bera test p-value is rather low, the p-value for the transformed model is much larger than that of the untransformed model, and furthermore, based on the residual plot we may conclude that there is ultimately no need for further transformation as the scatter is overall random around the horizontal line at 0, indicating a more normal distribution.\n\n\nCook’s Distance Plot\n\nlibrary(car)\ninfluenceIndexPlot(model_log, id=list(n=3), vars = \"Cook\", \n                   main = \"Cook's Distance Plot (Top 3 Influential Points)\")\n\n\n\n\n\n\n\n\nThe Cook’s Distance Plot also identifies the same infuential points as the QQ Plot, 868 and 909 as well as point 170.\n\n\nOutlier and Leverage Diagnostics\n\nlibrary(olsrr)\n\n\nAttaching package: 'olsrr'\n\n\nThe following object is masked from 'package:datasets':\n\n    rivers\n\nols_plot_resid_lev(model_log)\n\n\n\n\n\n\n\n\nConsidering the transformed residual plot, qq plot, Cook’s distance plot, and the Outlier and Leverage Diagnostics, while there are outliers such as points 868, 909, and 170, they would not need to be removed. In the qqplot the outliers outlined are within the 95% confidence interval in addition to the Bonferroni outlier test which demonstrated that these outliers were not significant. Furthermore, as the residual plot of the candidate model is fairly scattered, suggesting that the model assumptions are upheld, the inclusion of the outliers does not appear to distort the overall randomness of the residuals. Therefore, it is reasonable to retain the outliers in this analysis.\n\n\nAIC for Model Selection\n\nlibrary(broom)\nlibrary(POE5Rdata)\n\n\nAttaching package: 'POE5Rdata'\n\n\nThe following object is masked from 'package:datasets':\n\n    euro\n\nmodel_log.mod1 &lt;- lm(log_Weight~ log_BMI, data = gym_members_exercise_tracking)\nmodel_log.mod2 &lt;- lm(log_Weight~ log_BMI + log_Height, \n                     data = gym_members_exercise_tracking)\nmodel_log.mod3 &lt;- lm(log_Weight~ log_BMI + log_Height + Fat_Percentage, \n                     data = gym_members_exercise_tracking)\nmodel_log.mod4 &lt;- lm(log_Weight~ log_BMI + log_Height + Fat_Percentage + \n                       `Water_Intake (liters)`, \n                     data = gym_members_exercise_tracking)\nmodel_log.mod5 &lt;- lm(log_Weight~ log_BMI + log_Height + Fat_Percentage + \n                       `Water_Intake (liters)` + `Session_Duration (hours)`, \n                data = gym_members_exercise_tracking)\nmodel_log.mod6 &lt;- lm(log_Weight~ log_BMI + log_Height + Fat_Percentage + \n                       `Water_Intake (liters)` + `Session_Duration (hours)` + \n                       Gender, \n                data = gym_members_exercise_tracking)\nmodel_log.mod7 &lt;- lm(log_Weight~ log_BMI + log_Height + Fat_Percentage + \n                       `Water_Intake (liters)` + `Session_Duration (hours)` + \n                       Gender + Workout_Type, \n                data = gym_members_exercise_tracking)\nAIC(model_log.mod1, model_log.mod2, model_log.mod3, model_log.mod4, \n    model_log.mod5, model_log.mod6, model_log.mod7)\n\n               df         AIC\nmodel_log.mod1  3   -994.0934\nmodel_log.mod2  4 -14647.7960\nmodel_log.mod3  5 -14645.8001\nmodel_log.mod4  6 -14643.8010\nmodel_log.mod5  7 -14641.9933\nmodel_log.mod6  8 -14643.7439\nmodel_log.mod7 11 -14638.4366\n\n\nAccording to the AIC model, the preferred model is Model 2, which only includes BMI and height.\n\n\nBIC for Model Selection\n\nlibrary(broom)\nlibrary(POE5Rdata)\nmodel_log.mod1 &lt;- lm(log_Weight~ log_BMI, data = gym_members_exercise_tracking)\nmodel_log.mod2 &lt;- lm(log_Weight~ log_BMI + log_Height, \n                     data = gym_members_exercise_tracking)\nmodel_log.mod3 &lt;- lm(log_Weight~ log_BMI + log_Height + Fat_Percentage, \n                     data = gym_members_exercise_tracking)\nmodel_log.mod4 &lt;- lm(log_Weight~ log_BMI + log_Height + Fat_Percentage + \n                       `Water_Intake (liters)`, \n                     data = gym_members_exercise_tracking)\nmodel_log.mod5 &lt;- lm(log_Weight~ log_BMI + log_Height + Fat_Percentage + \n                       `Water_Intake (liters)` + `Session_Duration (hours)`, \n                data = gym_members_exercise_tracking)\nmodel_log.mod6 &lt;- lm(log_Weight~ log_BMI + log_Height + Fat_Percentage + \n                       `Water_Intake (liters)` + `Session_Duration (hours)` + \n                       Gender, \n                data = gym_members_exercise_tracking)\nmodel_log.mod7 &lt;- lm(log_Weight~ log_BMI + log_Height + Fat_Percentage + \n                       `Water_Intake (liters)` + `Session_Duration (hours)` + \n                       Gender + Workout_Type, \n                data = gym_members_exercise_tracking)\nBIC(model_log.mod1, model_log.mod2, model_log.mod3, model_log.mod4, \n    model_log.mod5, model_log.mod6, model_log.mod7)\n\n               df         BIC\nmodel_log.mod1  3   -979.4523\nmodel_log.mod2  4 -14628.2745\nmodel_log.mod3  5 -14621.3982\nmodel_log.mod4  6 -14614.5187\nmodel_log.mod5  7 -14607.8306\nmodel_log.mod6  8 -14604.7008\nmodel_log.mod7 11 -14584.7524\n\n\nAccording to the BIC model, the preferred model is Model 2, which only includes BMI and height.\n\n\nMulticollinearity\n\nlibrary(car)\nvif(model_log)\n\n                               GVIF Df GVIF^(1/(2*Df))\nlog_BMI                    1.353499  1        1.163400\nlog_Height                 1.886661  1        1.373558\nFat_Percentage             2.258214  1        1.502735\n`Water_Intake (liters)`    2.354351  1        1.534389\n`Session_Duration (hours)` 1.743597  1        1.320453\nGender                     3.006329  1        1.733877\nWorkout_Type               1.022472  3        1.003711\n\n\nUsing threshold 4, we keep all variables.\n\n\nCompare Transformed Model to Original\n\nmodel_original &lt;- lm(`Weight (kg)` ~ BMI + `Height (m)` + Fat_Percentage + \n                       `Water_Intake (liters)` + `Session_Duration (hours)` + \n                       Gender + Workout_Type, \n                     data = gym_members_exercise_tracking)\n\nAIC(model_original, model_log)\n\n               df        AIC\nmodel_original 11   4559.816\nmodel_log      11 -14638.437\n\nBIC(model_original, model_log)\n\n               df        BIC\nmodel_original 11   4613.501\nmodel_log      11 -14584.752\n\n\nA Lower AIC and BIC Indicate a better fit compared to the original model.\n\npredicted_log_weight &lt;- predict(model_log, gym_members_exercise_tracking)\nactual_log_weight &lt;- gym_members_exercise_tracking$log_Weight\n\nplot(predicted_log_weight, actual_log_weight, main = \"Predicted vs Actual\n     (Log-Transformed Weight)\",\n     xlab = \"Predicted Log Weight\", ylab = \"Actual Log Weight\", col = \"blue\")\nabline(0, 1, col = \"red\") \n\n\n\n\n\n\n\n\nAlso by visualizing the data we can see that the transformed model does a good job. I tested for multicollinearity by using the VIF method, and based on the threshold of 4, we do not need to remove any of the variables. Furthermore, comparing the AIC and BIC values for the untransformed data to the transformed candidate model, the canidate model seems to have a much better fit as AIC and BIC value is far smaller. Finally, we compared our candidate model to the actual data which was fairly aligned indicating that the model is a good fit of the data.\n\n\nModel Misspecification Ramsey Reset\n\nlibrary(car)\nlibrary(POE5Rdata)\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\nreset_test &lt;- resettest(model_log, power = 2:3, type = \"fitted\")\nprint(reset_test)\n\n\n    RESET test\n\ndata:  model_log\nRESET = 1.1687, df1 = 2, df2 = 961, p-value = 0.3112\n\n\nBy running the RESET test, we see that the P-Value of 0.3112 is large enough at the 95% confidence level to fail to reject the null, so a nonlinear term is not needed. This indicates that there is no strong evidence to suggest that important unobserved variables are omitted from the model. Moreover, this implies that the candidate model is likely appropriate for understanding the relationship between the response variable, weight,and the predictor variables.\n\n\nTest for Interaction Terms\n\nlibrary(POE5Rdata)\nlibrary(effects)\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\nmodel_log1 &lt;- lm(log_Weight~log_BMI+log_Height+Fat_Percentage+\n                   `Water_Intake (liters)`+`Session_Duration (hours)`+ `Gender` + `Workout_Type` + log_BMI:log_Height, data = gym_members_exercise_tracking)\nmodel_log2 &lt;- lm(log_Weight~log_BMI+log_Height+Fat_Percentage+\n                   `Water_Intake (liters)`+`Session_Duration (hours)`+ `Gender` + `Workout_Type`+ log_Height:Fat_Percentage, data = gym_members_exercise_tracking)\nmodel_log3 &lt;- lm(log_Weight~log_BMI+log_Height+Fat_Percentage+\n                   `Water_Intake (liters)`+`Session_Duration (hours)`+ `Gender` + `Workout_Type` + `Water_Intake (liters)`:Fat_Percentage, data = gym_members_exercise_tracking)\nmodel_log4 &lt;- lm(log_Weight~log_BMI+log_Height+Fat_Percentage+\n                   `Water_Intake (liters)`+`Session_Duration (hours)`+ `Gender` + `Workout_Type` + `Gender`:`Workout_Type` , data = gym_members_exercise_tracking)\n\nsummary(model_log1)\n\n\nCall:\nlm(formula = log_Weight ~ log_BMI + log_Height + Fat_Percentage + \n    `Water_Intake (liters)` + `Session_Duration (hours)` + Gender + \n    Workout_Type + log_BMI:log_Height, data = gym_members_exercise_tracking)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-3.832e-04 -9.782e-05  5.300e-06  9.703e-05  3.803e-04 \n\nCoefficients:\n                             Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept)                -4.785e-04  4.500e-04   -1.063   0.2879    \nlog_BMI                     1.000e+00  1.379e-04 7252.725   &lt;2e-16 ***\nlog_Height                  2.001e+00  7.940e-04 2520.028   &lt;2e-16 ***\nFat_Percentage             -9.340e-08  1.002e-06   -0.093   0.9257    \n`Water_Intake (liters)`     9.812e-06  1.067e-05    0.920   0.3579    \n`Session_Duration (hours)` -2.831e-06  1.606e-05   -0.176   0.8601    \nGenderMale                 -3.046e-05  1.461e-05   -2.085   0.0373 *  \nWorkout_TypeHIIT           -4.689e-06  1.200e-05   -0.391   0.6961    \nWorkout_TypeStrength        3.581e-06  1.155e-05    0.310   0.7565    \nWorkout_TypeYoga            3.955e-06  1.178e-05    0.336   0.7371    \nlog_BMI:log_Height         -2.459e-04  2.457e-04   -1.001   0.3172    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0001301 on 962 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:      1 \nF-statistic: 4.432e+08 on 10 and 962 DF,  p-value: &lt; 2.2e-16\n\nsummary(model_log2)\n\n\nCall:\nlm(formula = log_Weight ~ log_BMI + log_Height + Fat_Percentage + \n    `Water_Intake (liters)` + `Session_Duration (hours)` + Gender + \n    Workout_Type + log_Height:Fat_Percentage, data = gym_members_exercise_tracking)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-3.832e-04 -9.723e-05  5.990e-06  9.800e-05  3.976e-04 \n\nCoefficients:\n                             Estimate Std. Error   t value Pr(&gt;|t|)    \n(Intercept)                 6.600e-05  1.539e-04     0.429   0.6681    \nlog_BMI                     1.000e+00  1.845e-05 54189.753   &lt;2e-16 ***\nlog_Height                  2.000e+00  2.338e-04  8553.943   &lt;2e-16 ***\nFat_Percentage             -4.460e-06  5.094e-06    -0.876   0.3815    \n`Water_Intake (liters)`     9.809e-06  1.067e-05     0.919   0.3581    \n`Session_Duration (hours)` -2.400e-06  1.606e-05    -0.149   0.8812    \nGenderMale                 -2.948e-05  1.452e-05    -2.030   0.0427 *  \nWorkout_TypeHIIT           -5.365e-06  1.199e-05    -0.448   0.6546    \nWorkout_TypeStrength        3.012e-06  1.156e-05     0.261   0.7944    \nWorkout_TypeYoga            3.044e-06  1.177e-05     0.259   0.7960    \nlog_Height:Fat_Percentage   8.036e-06  9.210e-06     0.873   0.3831    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0001301 on 962 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:      1 \nF-statistic: 4.431e+08 on 10 and 962 DF,  p-value: &lt; 2.2e-16\n\nsummary(model_log3)\n\n\nCall:\nlm(formula = log_Weight ~ log_BMI + log_Height + Fat_Percentage + \n    `Water_Intake (liters)` + `Session_Duration (hours)` + Gender + \n    Workout_Type + `Water_Intake (liters)`:Fat_Percentage, data = gym_members_exercise_tracking)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-3.816e-04 -9.735e-05  5.100e-06  9.679e-05  4.013e-04 \n\nCoefficients:\n                                         Estimate Std. Error   t value Pr(&gt;|t|)\n(Intercept)                            -5.675e-05  1.310e-04    -0.433   0.6649\nlog_BMI                                 1.000e+00  1.846e-05 54164.065   &lt;2e-16\nlog_Height                              2.000e+00  7.801e-05 25638.184   &lt;2e-16\nFat_Percentage                          6.245e-07  3.697e-06     0.169   0.8659\n`Water_Intake (liters)`                 1.624e-05  3.376e-05     0.481   0.6306\n`Session_Duration (hours)`             -2.959e-06  1.624e-05    -0.182   0.8555\nGenderMale                             -2.824e-05  1.454e-05    -1.942   0.0524\nWorkout_TypeHIIT                       -5.392e-06  1.200e-05    -0.449   0.6532\nWorkout_TypeStrength                    3.395e-06  1.155e-05     0.294   0.7689\nWorkout_TypeYoga                        3.451e-06  1.177e-05     0.293   0.7694\nFat_Percentage:`Water_Intake (liters)` -2.544e-07  1.246e-06    -0.204   0.8383\n                                          \n(Intercept)                               \nlog_BMI                                ***\nlog_Height                             ***\nFat_Percentage                            \n`Water_Intake (liters)`                   \n`Session_Duration (hours)`                \nGenderMale                             .  \nWorkout_TypeHIIT                          \nWorkout_TypeStrength                      \nWorkout_TypeYoga                          \nFat_Percentage:`Water_Intake (liters)`    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0001301 on 962 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:      1 \nF-statistic: 4.427e+08 on 10 and 962 DF,  p-value: &lt; 2.2e-16\n\nsummary(model_log4)\n\n\nCall:\nlm(formula = log_Weight ~ log_BMI + log_Height + Fat_Percentage + \n    `Water_Intake (liters)` + `Session_Duration (hours)` + Gender + \n    Workout_Type + Gender:Workout_Type, data = gym_members_exercise_tracking)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-3.801e-04 -9.750e-05  5.400e-06  9.779e-05  3.999e-04 \n\nCoefficients:\n                                  Estimate Std. Error   t value Pr(&gt;|t|)    \n(Intercept)                     -3.644e-05  9.693e-05    -0.376    0.707    \nlog_BMI                          1.000e+00  1.852e-05 53988.484   &lt;2e-16 ***\nlog_Height                       2.000e+00  7.815e-05 25591.518   &lt;2e-16 ***\nFat_Percentage                  -1.247e-07  1.003e-06    -0.124    0.901    \n`Water_Intake (liters)`          9.815e-06  1.068e-05     0.919    0.358    \n`Session_Duration (hours)`      -2.958e-06  1.609e-05    -0.184    0.854    \nGenderMale                      -2.049e-05  2.002e-05    -1.024    0.306    \nWorkout_TypeHIIT                -2.160e-06  1.715e-05    -0.126    0.900    \nWorkout_TypeStrength             1.422e-05  1.656e-05     0.859    0.391    \nWorkout_TypeYoga                 4.718e-06  1.722e-05     0.274    0.784    \nGenderMale:Workout_TypeHIIT     -6.286e-06  2.398e-05    -0.262    0.793    \nGenderMale:Workout_TypeStrength -2.103e-05  2.306e-05    -0.912    0.362    \nGenderMale:Workout_TypeYoga     -3.148e-06  2.355e-05    -0.134    0.894    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0001302 on 960 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:      1 \nF-statistic: 3.685e+08 on 12 and 960 DF,  p-value: &lt; 2.2e-16\n\n\nUsing the p-value of coefficients, we see that the model is not statistically significant among the interaction terms, so we won’t need to include them.\n\n\nBootstrapping Log Transformed Model with Interaction Variable\n\nlibrary(boot)\n\n\nAttaching package: 'boot'\n\n\nThe following object is masked from 'package:POE5Rdata':\n\n    tuna\n\n\nThe following object is masked from 'package:car':\n\n    logit\n\nlibrary(ggplot2)\n\nbootstrap_function &lt;- function(data, indices) {\n  resampled_data &lt;- data[indices, ]\n\n  model_resampled &lt;- lm(log_Weight ~ log_BMI * Fat_Percentage + log_Height + \n                        `Water_Intake (liters)` + `Session_Duration (hours)`, \n                        data = resampled_data)\n  return(coef(model_resampled))  }\nset.seed(123) \nboot_results &lt;- boot(data = gym_members_exercise_tracking, statistic = bootstrap_function, R = 1000)\nboot_results\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = gym_members_exercise_tracking, statistic = bootstrap_function, \n    R = 1000)\n\n\nBootstrap Statistics :\n         original        bias     std. error\nt1* -5.882657e-04 -8.554770e-06 3.014201e-04\nt2*  1.000187e+00  2.631089e-06 8.886319e-05\nt3*  2.423869e-05  2.404932e-07 1.133134e-05\nt4*  1.999968e+00  8.871506e-07 6.440787e-05\nt5* -1.338986e-06 -5.953335e-08 9.266424e-06\nt6*  7.157967e-06 -1.077169e-07 1.619223e-05\nt7* -7.503450e-06 -7.659321e-08 3.458305e-06\n\n\n\ncolnames(boot_results$t) &lt;- names(coef(lm(log_Weight ~ log_BMI * Fat_Percentage + log_Height + \n                                         `Water_Intake (liters)` + `Session_Duration (hours)`, \n                                         data = gym_members_exercise_tracking)))\n\ninteraction_coefficients &lt;- boot_results$t[, \"log_BMI:Fat_Percentage\"]\n\nhist(interaction_coefficients, \n     main = \"Bootstrap Distribution of log_BMI:Fat_Percentage Interaction Coefficient\", \n     xlab = \"Interaction Coefficient (log_BMI:Fat_Percentage)\", \n     col = \"lightblue\", \n     border = \"black\", \n     breaks = 30)\n\n\n\n\n\n\n\n\nThe Histogram looks fairly symmetric and normally distributed. Based on this histogram, the sampling distribution of the predictors are overall approximately normal, which suggests that the inference methods relying on normality may still be reasonably robust.\n\n\nCross Validation\n\nif (!require(caret)) install.packages(\"caret\")\n\nLoading required package: caret\n\n\nLoading required package: lattice\n\n\n\nAttaching package: 'lattice'\n\n\nThe following object is masked from 'package:boot':\n\n    melanoma\n\nlibrary(caret)\n\n\nset.seed(123) \ncv_control &lt;- trainControl(method = \"cv\", number = 10)\n\ncv_model &lt;- train(log_Weight ~ log_BMI * Fat_Percentage + log_Height + \n                  `Water_Intake (liters)` + `Session_Duration (hours)`, \n                  data = gym_members_exercise_tracking, \n                  method = \"lm\",  \n                  trControl = cv_control)\nprint(cv_model)\n\nLinear Regression \n\n973 samples\n  5 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 875, 874, 876, 876, 876, 876, ... \nResampling results:\n\n  RMSE          Rsquared   MAE         \n  0.0001299402  0.9999998  0.0001083176\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\ncat(\"Cross-Validated RMSE:\", cv_model$results$RMSE, \"\\n\")\n\nCross-Validated RMSE: 0.0001299402 \n\ncat(\"Cross-Validated R-Squared:\", cv_model$results$Rsquared, \"\\n\")\n\nCross-Validated R-Squared: 0.9999998 \n\n\nRMSE of 0.0001299402 suggests that the model’s predictions of log_Weight deviate only slightly from the observed values. An R-squared value of 0.9999998 means that nearly all the variability in log_Weight is explained by the predictors, including the interaction term. A MAE of 0.0001083176 is very close to zero, further confirming the model’s strong predictive power.\n\n\nCheck for Overfitting since R-squared value is so high\n\nset.seed(123)\ntrain_indices &lt;- createDataPartition(gym_members_exercise_tracking$log_Weight, p = 0.8, list = FALSE)\ntraining_data &lt;- gym_members_exercise_tracking[train_indices, ]\ntesting_data &lt;- gym_members_exercise_tracking[-train_indices, ]\n\nmodel_train &lt;- lm(log_Weight ~ log_BMI * Fat_Percentage + log_Height + \n                  `Water_Intake (liters)` + `Session_Duration (hours)`, \n                  data = training_data)\n\ntrain_predictions &lt;- predict(model_train, newdata = training_data)\ntrain_rmse &lt;- sqrt(mean((train_predictions - training_data$log_Weight)^2))\ntrain_r2 &lt;- 1 - sum((train_predictions - training_data$log_Weight)^2) / \n                sum((training_data$log_Weight - mean(training_data$log_Weight))^2)\n\ncat(\"Training RMSE:\", train_rmse, \"\\n\")\n\nTraining RMSE: 0.0001309695 \n\ncat(\"Training R²:\", train_r2, \"\\n\")\n\nTraining R²: 0.9999998 \n\ntest_predictions &lt;- predict(model_train, newdata = testing_data)\ntest_rmse &lt;- sqrt(mean((test_predictions - testing_data$log_Weight)^2))\ntest_r2 &lt;- 1 - sum((test_predictions - testing_data$log_Weight)^2) / \n               sum((testing_data$log_Weight - mean(testing_data$log_Weight))^2)\n\ncat(\"Test RMSE:\", test_rmse, \"\\n\")\n\nTest RMSE: 0.0001237238 \n\ncat(\"Test R²:\", test_r2, \"\\n\")\n\nTest R²: 0.9999998 \n\n\nThe slightly lower RMSE on the test set (0.0001237238) suggests that the model generalizes very well to unseen data and does not overfit. The identical R² values for both the training and test sets suggest that the model performs equally well on unseen data as it does on the data it was trained on, confirming no signs of overfitting.\n\n\nFavorite Specification\nIn our analysis, using the Jarque Bera test, the qq plot, and the residual plot of the candidate model, we found that there is no need for further transformations on our model. Based on the Cook’s Distance plot, the Outlier and Leverage Diagnostic plot, with inferences from the residual plot, we noted that while there are outliers in our model, they do not need to be removed. Using the AIC and BIC model, along with the VIF method, we tested for multicollinearity and noted that no variables needed to be removed. To test model misspecification, we ran the RESET test and as the p-value was not statistically significant, we concluded that nonlinear terms were not needed in the model. We additionally tested interactions between variables and found that there were no significant interactions to include amongst the ones tested. Finally, using the boostrap method, the resulting histogram was overall normally distributed resulting in a reasonable amount of robustness. A preferred form of analysis was the RESET test, as it’s p-value indicated whether nonlinear terms were needed in the model, and furthermore if there were any unobserved omitted variables (which may have been the cause if the p-value was below 0.05). This test incorporates our understanding of the p-value to test specific characteristics on the model and encourage changes such as a quardratic variable or to think further about unobserved variables when the p-value is significant.\n\n\nConclusion\n\nsummary(model_log)\n\n\nCall:\nlm(formula = log_Weight ~ log_BMI + log_Height + Fat_Percentage + \n    `Water_Intake (liters)` + `Session_Duration (hours)` + Gender + \n    Workout_Type, data = gym_members_exercise_tracking)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-3.820e-04 -9.716e-05  5.880e-06  9.825e-05  4.012e-04 \n\nCoefficients:\n                             Estimate Std. Error   t value Pr(&gt;|t|)    \n(Intercept)                -3.866e-05  9.642e-05    -0.401   0.6885    \nlog_BMI                     1.000e+00  1.845e-05 54203.765   &lt;2e-16 ***\nlog_Height                  2.000e+00  7.789e-05 25676.470   &lt;2e-16 ***\nFat_Percentage             -1.021e-07  1.002e-06    -0.102   0.9188    \n`Water_Intake (liters)`     9.701e-06  1.067e-05     0.909   0.3633    \n`Session_Duration (hours)` -2.476e-06  1.606e-05    -0.154   0.8775    \nGenderMale                 -2.850e-05  1.448e-05    -1.969   0.0493 *  \nWorkout_TypeHIIT           -5.324e-06  1.199e-05    -0.444   0.6570    \nWorkout_TypeStrength        3.372e-06  1.155e-05     0.292   0.7703    \nWorkout_TypeYoga            3.388e-06  1.176e-05     0.288   0.7734    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0001301 on 963 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:      1 \nF-statistic: 4.924e+08 on 9 and 963 DF,  p-value: &lt; 2.2e-16\n\n\nUpon building and testing our model through various forms of analysis, we may now use it to help answer some of our motivating questions. Using the estimated values shown in the above summary of our model, we are able to observe whether there is a linear relationship between our reponse variable weight, and each of the seven predictors. We notice that BMI and Height have a positive linear relationship with weight therefore the marginal effect from an increase in either of these variables will have an increase in weight as well. This may be due to the fact that BMI and Height have a direct relationship with weight as BMI times Height (m)^2 is equal to weight, indicating a direct positive relationship. BMI and Height therefore seem to have a stronger relationship to weight, which is further assured by AIC and BIC models in which the preferred model included only BMI and Height. Additionally, from our model, we observe that Water Intake (liters), Strength Workouts and Yoga Workout, have a positive linear relationship with weight (based on their marginal effects), suggesting that increased water intake, a higher frequency of strength and yoga workouts may be associated with higher weight when considered marginally. Fat percentage, gender (male), Session duration, and HIIT Workout have negative marginal effects in relation to weight. This suggests that these variables are associated with reductions in weight when considered marginally."
  },
  {
    "objectID": "projects/04-project.html",
    "href": "projects/04-project.html",
    "title": "Economics 104 - Swiss Labor Market Dataset",
    "section": "",
    "text": "Briefly discuss the question you are trying to answer with your model.\nWe are using the Swiss Labor Market Participation Dataset from the 1981 Swiss Health Survey (SOMIPOPS) found in the Journal of Applied Economietrics Data Archive (Gerfin, 1996). The question we will be looking to answer is: “What factors influence whether an individual participates in the labor force in Switzerland?” Our (binary) dependent variable is participation, indicating whether an individual is active in the labor force (yes or no). The model includes six predictors: income (log of non-labor income), age (in decades), education (years of formal education), youngkids (number of children under 7), oldkids (number of children 7 or older), and foreign (whether the individual is a foreginer or Swiss). The model includes 2 categorical variables, participation (the dependent variable) and foreign, and 5 continuous variables (income, age, education, youngkids, and oldkids). The source of the data is: Gerfin, M. (1996). Parametric and Semi-Parametric Estimation of the Binary Response Model of Labour Market Participation, Journal of Applied Econometrics, 11, 321–339, available at http://qed.econ.queensu.ca/jae/1996-v11.3/gerfin/."
  },
  {
    "objectID": "projects/04-project.html#a.",
    "href": "projects/04-project.html#a.",
    "title": "Economics 104 - Swiss Labor Market Dataset",
    "section": "",
    "text": "Briefly discuss the question you are trying to answer with your model.\nWe are using the Swiss Labor Market Participation Dataset from the 1981 Swiss Health Survey (SOMIPOPS) found in the Journal of Applied Economietrics Data Archive (Gerfin, 1996). The question we will be looking to answer is: “What factors influence whether an individual participates in the labor force in Switzerland?” Our (binary) dependent variable is participation, indicating whether an individual is active in the labor force (yes or no). The model includes six predictors: income (log of non-labor income), age (in decades), education (years of formal education), youngkids (number of children under 7), oldkids (number of children 7 or older), and foreign (whether the individual is a foreginer or Swiss). The model includes 2 categorical variables, participation (the dependent variable) and foreign, and 5 continuous variables (income, age, education, youngkids, and oldkids). The source of the data is: Gerfin, M. (1996). Parametric and Semi-Parametric Estimation of the Binary Response Model of Labour Market Participation, Journal of Applied Econometrics, 11, 321–339, available at http://qed.econ.queensu.ca/jae/1996-v11.3/gerfin/."
  },
  {
    "objectID": "projects/04-project.html#b.",
    "href": "projects/04-project.html#b.",
    "title": "Economics 104 - Swiss Labor Market Dataset",
    "section": "b.",
    "text": "b.\nProvide a descriptive analysis of your variables.\n\nlibrary(AER)\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\nLoading required package: lmtest\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nLoading required package: sandwich\n\n\nLoading required package: survival\n\ndata(\"SwissLabor\")\nSwissLabor &lt;- SwissLabor\n\nParticipation Percentages\n\ntable(SwissLabor$participation)\n\n\n no yes \n471 401 \n\nprop.table(table(SwissLabor$participation)) * 100\n\n\n      no      yes \n54.01376 45.98624 \n\n\nThe binary dependent variable participation indicates whether an individual participates in the Swiss laber force in the SwissLabor dataset. From the summary table, we see that about 54.01% of individuals participate in the Swiss labor force. This shows that majority of the sample participate in the labor force.\nIncome Summary\n\nlibrary(AER)\nlibrary(pastecs)\ndata(\"SwissLabor\")\nstat.desc(SwissLabor$income)\n\n     nbr.val     nbr.null       nbr.na          min          max        range \n8.720000e+02 0.000000e+00 0.000000e+00 7.186901e+00 1.237565e+01 5.188749e+00 \n         sum       median         mean      SE.mean CI.mean.0.95          var \n9.317815e+03 1.064313e+01 1.068557e+01 1.396863e-02 2.741611e-02 1.701470e-01 \n     std.dev     coef.var \n4.124888e-01 3.860242e-02 \n\nsummary(SwissLabor$income)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  7.187  10.472  10.643  10.686  10.887  12.376 \n\n\nHistogram of Income with Density Plot\n\nhist(SwissLabor$income, col = \"lightblue\", breaks = 10, xlab = \"Income (log of non-labor income)\", \n     main = \"Histogram of Income\", prob = TRUE, xlim = c(7, 13))\nlines(density(SwissLabor$income), lwd = 2, col = \"red\")\nlines(density(SwissLabor$income, adjust = .5), lwd = 2, col = \"blue\", \n      type = 'l', lty = 2)\nrug(SwissLabor$income)\n\n\n\n\n\n\n\n\nThe histogram of the Income (log of non-labor income) indicates that the distribution of the data is left skewed. Therefore we would be looking to use the median as opposed to the mean to measure the central tendency of the data. There is a large range of values, spanning from 7.187 to 12.376. The density plot illustrates a smooth, continuous estimate of the probability density function (PDF) of the data. The blue dotted line is a more precise depiction of the data estimates. It provides a clearer view of the distribution’s shape, allowing you to see peaks and valleys.\nBoxplot of Income\n\nboxplot(SwissLabor$income, ylab = \"Income (log of non-labor income)\", \n        main = \"Boxplot of Income\", col = \"lightblue\")\n\n\n\n\n\n\n\n\nFrom the boxplot of Income (log of non-labor income), we’re able to solidify a few key insights about the data that we learned through prior tools and figures. We’re able to see that the median of the data is between 10 and 11, which agrees with the true median of 10.643. We’re also able to see the rather small standard deviation of about 4.12e^-1. Both of the whiskers seem to be of similar size with outliers on either end. However, there is a potential outlier seeming to be at the minimum value, influencing the skew of the distribution. The appropriate values of Q1 and Q3 are also displayed, at 10.472 and 10.887, respectively. In addition, the box plot illustrates the min and max of the data, showing a difference of about 5 between the two.\nAge Summary\n\nlibrary(pastecs)\nstat.desc(SwissLabor$age)\n\n     nbr.val     nbr.null       nbr.na          min          max        range \n8.720000e+02 0.000000e+00 0.000000e+00 2.000000e+00 6.200000e+00 4.200000e+00 \n         sum       median         mean      SE.mean CI.mean.0.95          var \n3.484100e+03 3.900000e+00 3.995528e+00 3.573248e-02 7.013183e-02 1.113378e+00 \n     std.dev     coef.var \n1.055167e+00 2.640871e-01 \n\nsummary(SwissLabor$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.000   3.200   3.900   3.996   4.800   6.200 \n\n\nHistogram of Age with Density Plot\n\nhist(SwissLabor$age, col = \"coral\", breaks = 10, xlab = \"Age (in decades)\", \n     main = \"Histogram of Age\", prob = TRUE, xlim = c(1,7))\nlines(density(SwissLabor$age), lwd = 2, col = \"red\")\nlines(density(SwissLabor$age, adjust = .5), lwd = 2, col = \"blue\", \n      type = 'l', lty = 2)\nrug(SwissLabor$age)\n\n\n\n\n\n\n\n\nThe histogram of Age (in decades), has a somewhat central peak and overall looks to behave more normally. To measure the central tendency of the data, we may use either median or mean as there doesn’t seem to be heavy skews or deviations from normality. There is a large range of values, spanning from 2 to 6.2. The density plot illustrates a smooth, continuous estimate of the probability density function (PDF) of the data. The blue dotted line is a more precise depiction of the data estimates. It provides a clearer view of the distribution’s shape, allowing you to see smaller peaks and valleys.\nBoxplot of Age\n\nboxplot(SwissLabor$age, ylab = \"Age (in decades)\", \n        main = \"Boxplot of Age\", col = \"coral\")\n\n\n\n\n\n\n\n\nFrom the boxplot of Age (in decades), we are able to solidify a few key insights about the data that we learned through prior tools and figures. We are able to see that the median of the data is between 3 and 5, which agrees with the true median of 3.9. We are also able to see that the standard deviation of about 1.055. Additionally, both whiskers seem to be of similar size, which no indication of outliers. The appropriate values of Q1 and Q3 are also displayed, at 3.2 and 4.8, respectively. The box plot also illustrates the min and max of the data, showing that the difference is quite large (in decades) between the two.\nEducation Summary\n\nstat.desc(SwissLabor$education)\n\n     nbr.val     nbr.null       nbr.na          min          max        range \n 872.0000000    0.0000000    0.0000000    1.0000000   21.0000000   20.0000000 \n         sum       median         mean      SE.mean CI.mean.0.95          var \n8116.0000000    9.0000000    9.3073394    0.1028207    0.2018053    9.2188669 \n     std.dev     coef.var \n   3.0362587    0.3262220 \n\nsummary(SwissLabor$education)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   8.000   9.000   9.307  12.000  21.000 \n\n\nHistogram of Education with Density Plot\n\nhist(SwissLabor$education, col = \"mediumaquamarine\", breaks = 10, \n     xlab = \"Education (years of formal education)\", \n     main = \"Histogram of Education\", \n     prob = TRUE, xlim = c(0, 25))\nlines(density(SwissLabor$education), lwd = 2, col = \"red\")\nlines(density(SwissLabor$education, adjust = .5), lwd = 2, col = \"blue\", \n      type = 'l', lty = 2)\nrug(SwissLabor$education)\n\n\n\n\n\n\n\n\nThe histogram of Education (years of formal schooling), indicates that the distribution is slightly right-skewed. So, we would be looking to use the median as opposed to the mean to measure the central tendency of the data. There is a large range of values, spanning from 1 to 21. The density plot illustrates a smooth, continuous estimate of the probability density function (PDF) of the data. The blue dotted line is a more precise depiction of the data estimates. It provides a clearer view of the distribution’s shape, allowing you to see smaller peaks and valleys.\nBoxplot of Education\n\nboxplot(SwissLabor$education, ylab = \"Education (years of schooling)\", \n        main = \"Boxplot of Education\", col = \"mediumaquamarine\")\n\n\n\n\n\n\n\n\nFrom the boxplot of Education (years of schooling), we are able to solidify a few key insights about the data that we learned about through prior tools and figures. We’re able to see that the median of the data is between 8 and 12, which agrees with the true median of 9. We are also able to see the standard deviation of about 3.03. Additionally, both of the whiskers looks to be of similar size, with a few outliers above the median, suggesting the slight right-skew of the distribution. The appropriate values of Q1 and Q3 are also displayed, at 8 and 12. In addition, the box plot illustrates the min and max of the data, showing that the difference rather large between the two. There are a few outliers shown in the box plot on either side of the median (though moreso above the median), matching our previous findings as well.\nYoung kids Summary\n\nstat.desc(SwissLabor$youngkids)\n\n     nbr.val     nbr.null       nbr.na          min          max        range \n872.00000000 665.00000000   0.00000000   0.00000000   3.00000000   3.00000000 \n         sum       median         mean      SE.mean CI.mean.0.95          var \n272.00000000   0.00000000   0.31192661   0.02075440   0.04073447   0.37560960 \n     std.dev     coef.var \n  0.61286997   1.96478903 \n\nsummary(SwissLabor$youngkids)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0000  0.0000  0.3119  0.0000  3.0000 \n\n\nHistogram of Young kids with Density Plot\n\nhist(SwissLabor$youngkids, col = \"indianred\", breaks = 10, \n     xlab = \"Young Kids (number of kids 7 or younger)\", \n     main = \"Histogram of Young Kids\", prob = TRUE, xlim = c(0, 4))\nlines(density(SwissLabor$youngkids), lwd = 2, col = \"red\")\nlines(density(SwissLabor$youngkids, adjust = .5), lwd = 2, col = \"blue\", \n      type = 'l', lty = 2)\nrug(SwissLabor$youngkids)\n\n\n\n\n\n\n\n\nThe histogram of the Young Kids (number of kids individuals have that are 7 or younger), indicates that the distribution is extremely right-skewed. Therefore, we would be looking to use the median as opposed to the mean to measure the central tendency of the data. There is a shorter range of values, spanning from 0 to 3. The density plot illustrates a smooth, continuous estimate of the probability density function of the data. The blue dotted line is a more precise depiction of the data estimates. It provides a clearer view of the distribution’s shape, allowing you to see smaller peaks and valleys.\nBoxplot of Young kids\n\nboxplot(SwissLabor$youngkids, ylab = \"Young Kids\", \n        main = \"Boxplot of Young Kids\", col = \"indianred\")\n\n\n\n\n\n\n\n\nFrom the boxplot of youngkids (the number of kids an individual has that’s 7 or younger), we are able to solidify a few key insights about the data that we learned about through prior tools and figures. We are able to see that the median of the data is around 0, which agrees with the true median of 0. We are also able to see the standard deviation of about 0.612. Additionally, the comparatively large outliers above the median suggest the strong right-skew of the distribution. The values of Q1 and Q3 also seem to be around 0. Also, the box plot illustrates the min and max of the data, showing a difference of about 3 between the two. There are a few outliers shown in the box plot exceeding the standard deviation (all above the median), matching our previous findings as well.\nOld Kids Summary\n\nstat.desc(SwissLabor$oldkids)\n\n     nbr.val     nbr.null       nbr.na          min          max        range \n872.00000000 393.00000000   0.00000000   0.00000000   6.00000000   6.00000000 \n         sum       median         mean      SE.mean CI.mean.0.95          var \n857.00000000   1.00000000   0.98279817   0.03680323   0.07223338   1.18110445 \n     std.dev     coef.var \n  1.08678629   1.10580822 \n\nsummary(SwissLabor$oldkids)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0000  1.0000  0.9828  2.0000  6.0000 \n\n\nHistogram of Old Kids with Density Plot\n\nhist(SwissLabor$oldkids, col = \"sandybrown\", breaks = 10, \n     xlab = \"Old Kids (number of kids older than 7)\", \n     main = \"Histogram of Old Kids\", prob = TRUE, xlim = c(0, 25))\nlines(density(SwissLabor$oldkids), lwd = 2, col = \"red\")\nlines(density(SwissLabor$oldkids, adjust = .5), lwd = 2, col = \"blue\", \n      type = 'l', lty = 2)\nrug(SwissLabor$oldkids)\n\n\n\n\n\n\n\n\nThe histogram of the Old Kids (number of kids individuals have that are older than 7), indicates that the distribution is extremely right-skewed. Therefore, we would be looking to use the median as opposed to the mean to measure the central tendency of the data. There is a shorter range of values, spanning from 0 to 6. The density plot illustrates a smooth estimate with many of peaks and valleys of the probability density function of the data. The blue dotted line is a more precise depiction of the data estimatse. It provides a clearer view of the distribution’s shape, allowing you to see the many peaks and valleys.\nBoxplot of Old Kids\n\nboxplot(SwissLabor$oldkids, ylab = \"Old Kids\", \n        main = \"Boxplot of Old Kids\", col = \"sandybrown\")\n\n\n\n\n\n\n\n\nFrom the boxplot of Old Kids (the number of kids an individual has that are older than 7), we are able to solidify a few key insights about the data that we learned about through prior tools and figures. We are able to see that the median of the data lies between 0 and 2, which agrees with the true median of 1. We are also able to see that the standard deviation of about 1.08. Additionally, the upper whisker is much longer, suggesting that the data has a right skew with more high values or outliers above the median. The appropriate values of Q1 and Q3 are also displayed, at 0 and 2. In addition, the box plot illustrates the min and max of the data, showing a difference of 6 between the two. There are a few outliers shown in the box plot exceeding the standard deviation (above the median), matching our previous findings as well.\nForeign Percentages\n\ntable(SwissLabor$foreign)\n\n\n no yes \n656 216 \n\nprop.table(table(SwissLabor$foreign)) * 100\n\n\n      no      yes \n75.22936 24.77064 \n\n\nThe binary varialbe foreign indicates whether an individual is a foreigner (“yes) or not (”no”) in the SwissLabor dataset. From the summary table, we see that about 75.23% of individuals are not foreigners, while 24.8% are foreigners. This shows that majority of the sample consists of non-foreigners, while about a quarter of the sample are foreigners.\nCorrelation Matrix\n\n# excludes catagorical/binary variables participation and foreign\nnumeric_data &lt;- SwissLabor[, c(\"income\", \"age\", \"education\", \n                               \"youngkids\", \"oldkids\")]\ncor_matrix &lt;- cor(numeric_data, use = \"complete.obs\")\ncor_matrix\n\n               income         age   education   youngkids     oldkids\nincome     1.00000000  0.00618126  0.32734578 -0.01733803  0.13910359\nage        0.00618126  1.00000000 -0.14929401 -0.51909274 -0.11620515\neducation  0.32734578 -0.14929401  1.00000000  0.09834970 -0.03632097\nyoungkids -0.01733803 -0.51909274  0.09834970  1.00000000 -0.23842829\noldkids    0.13910359 -0.11620515 -0.03632097 -0.23842829  1.00000000\n\n\nThe correlation matrix displays the pairwise relationships betweeen several key continuous predictors in the dataset, including income (log of non-labor income), age (in decades), education (years of schooling), youngkids (number of kids less than 7), and oldkids (number of kids greater than 7). The participation and foreigner variable are not included due to their binary nature. A correlation close to 1 indicates a strong positive relationship, while a correlation close to -1 reflects a strong negative relationship. A correlation near 0 suggests no relationship. There appears to be a moderate positive correlation between education and income, indicating that individuals with higher education tend to have higher non-labor income. In contrast, there appears to be a moderately strong negative correlation between youngkids and age, indicating that younger individuals tend to have more young children. Additionally, there is a somewhat moderate negative correlation between youngkids and oldkids, suggesting that individuals with more children less than 7 tend to not have as many children greater than 7. Other correlations appear to be rather weak or negligible, suggesting low multicollinearity risks amongst predictors. Overall, the matrix helps identify which variables are more closely associated with one another, and informs further analysis and model building."
  },
  {
    "objectID": "projects/04-project.html#c.",
    "href": "projects/04-project.html#c.",
    "title": "Economics 104 - Swiss Labor Market Dataset",
    "section": "c. ",
    "text": "c. \nFit the three models below, and identify which model is your preferred one.\n\n# LPM \n# convert participation into a binary variable\nSwissLabor$part_num &lt;- ifelse(SwissLabor$participation == \"yes\", 1, 0)\n# fit LPM \nswiss.lpm &lt;- lm(part_num ~ income + age + education + youngkids \n                + oldkids + foreign, data = SwissLabor)\n\n# Probit\nswiss.probit &lt;- glm(participation ~ income + age + education + \n                      youngkids + oldkids + foreign, \n                    data = SwissLabor, family = binomial(link = \"probit\"))\n\n# Logit\nswiss.logit &lt;- glm(participation ~ income + age + education + youngkids + \n                     oldkids + foreign, \n                   data = SwissLabor, family = binomial(link = \"logit\"))\n\n\n# Compare the Models - AIC Tests\nAIC(swiss.lpm)\n\n[1] 1126.542\n\nAIC(swiss.probit)\n\n[1] 1066.983\n\nAIC(swiss.logit)\n\n[1] 1066.798\n\n# probit and logit are preferred with very similar results according to AIC tests\n\n\n# Compare the Models (probit and logit) - McFadden's R^2 (from scratch)\n\n# probit McFadden\nswiss.null_p &lt;- glm(participation ~ 1, data = SwissLabor, \n                    family = binomial(link = \"probit\"))\nlogLik.full_p &lt;- logLik(swiss.probit)\nlogLik.null_p &lt;- logLik(swiss.null_p)\n\nMcFadden.R2_p &lt;- 1 - (as.numeric(logLik.full_p) / as.numeric(logLik.null_p))\n\nMcFadden.R2_p\n\n[1] 0.1248651\n\n# logit McFadden\nswiss.null_l &lt;- glm(participation ~ 1, data = SwissLabor, \n                    family = binomial(link = \"logit\"))\nlogLik.full_l &lt;- logLik(swiss.logit)\nlogLik.null_l &lt;- logLik(swiss.null_l)\n\nMcFadden.R2_l &lt;- 1 - (as.numeric(logLik.full_l) / as.numeric(logLik.null_l))\n\nMcFadden.R2_l\n\n[1] 0.1250191\n\n\n\n# Confusion Matrix - Split Data\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\n\n\nAttaching package: 'caret'\n\n\nThe following object is masked from 'package:survival':\n\n    cluster\n\nset.seed(123)\nSwissLabor$part_num &lt;- ifelse(SwissLabor$participation == \"yes\", 1, 0)\nSwissLabor$foreign_num &lt;- ifelse(SwissLabor$foreign == \"yes\", 1, 0)\n# Split into 70% train, 30% test\ntrain_index &lt;- sample(c(TRUE, FALSE), nrow(SwissLabor), \n                      replace = TRUE, prob = c(0.7, 0.3))\n# Create datasets\ntrain &lt;- SwissLabor[train_index, ]\ntest &lt;- SwissLabor[!train_index, ]\n\n# LPM - Confusion Matrix\nset.seed(123)\nlpm_model &lt;- lm(part_num ~ income + age + education + \n                  youngkids + oldkids + foreign_num, data = train)\npred_lpm &lt;- predict(lpm_model, newdata = test)\nclass_lpm &lt;- ifelse(pred_lpm &gt;= 0.5, 1, 0)\nconfusionMatrix(as.factor(class_lpm), as.factor(test$part_num), positive = \"1\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 103  55\n         1  31  67\n                                          \n               Accuracy : 0.6641          \n                 95% CI : (0.6026, 0.7217)\n    No Information Rate : 0.5234          \n    P-Value [Acc &gt; NIR] : 3.568e-06       \n                                          \n                  Kappa : 0.3207          \n                                          \n Mcnemar's Test P-Value : 0.01313         \n                                          \n            Sensitivity : 0.5492          \n            Specificity : 0.7687          \n         Pos Pred Value : 0.6837          \n         Neg Pred Value : 0.6519          \n             Prevalence : 0.4766          \n         Detection Rate : 0.2617          \n   Detection Prevalence : 0.3828          \n      Balanced Accuracy : 0.6589          \n                                          \n       'Positive' Class : 1               \n                                          \n\n# Probit - Confusion Matrix\nprobit_model &lt;- glm(part_num ~ income + age + education + youngkids + \n                      oldkids + foreign_num, data = train, \n                    family = binomial(link = \"probit\"))\npred_probit &lt;- predict(probit_model, newdata = test, type = \"response\")\nclass_probit &lt;- ifelse(pred_probit &gt;= 0.5, 1, 0)\nconfusionMatrix(as.factor(class_probit), as.factor(test$part_num), positive = \"1\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 102  55\n         1  32  67\n                                         \n               Accuracy : 0.6602         \n                 95% CI : (0.5986, 0.718)\n    No Information Rate : 0.5234         \n    P-Value [Acc &gt; NIR] : 6.478e-06      \n                                         \n                  Kappa : 0.313          \n                                         \n Mcnemar's Test P-Value : 0.01834        \n                                         \n            Sensitivity : 0.5492         \n            Specificity : 0.7612         \n         Pos Pred Value : 0.6768         \n         Neg Pred Value : 0.6497         \n             Prevalence : 0.4766         \n         Detection Rate : 0.2617         \n   Detection Prevalence : 0.3867         \n      Balanced Accuracy : 0.6552         \n                                         \n       'Positive' Class : 1              \n                                         \n\n# Logit - Confusion Matrix\nlogit_model &lt;- glm(part_num ~ income + age + education + youngkids + \n                     oldkids + foreign_num, data = train, \n                   family = binomial(link = \"logit\"))\npred_logit &lt;- predict(logit_model, newdata = test, type = \"response\")\nclass_logit &lt;- ifelse(pred_logit &gt;= 0.5, 1, 0)\nconfusionMatrix(as.factor(class_logit), as.factor(test$part_num), positive = \"1\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 104  55\n         1  30  67\n                                          \n               Accuracy : 0.668           \n                 95% CI : (0.6066, 0.7254)\n    No Information Rate : 0.5234          \n    P-Value [Acc &gt; NIR] : 1.932e-06       \n                                          \n                  Kappa : 0.3283          \n                                          \n Mcnemar's Test P-Value : 0.009237        \n                                          \n            Sensitivity : 0.5492          \n            Specificity : 0.7761          \n         Pos Pred Value : 0.6907          \n         Neg Pred Value : 0.6541          \n             Prevalence : 0.4766          \n         Detection Rate : 0.2617          \n   Detection Prevalence : 0.3789          \n      Balanced Accuracy : 0.6626          \n                                          \n       'Positive' Class : 1               \n                                          \n\n\nBased on the model selection criteria, the logit model is preferred. Both the probit and logit models yield similarly low AIC values, indicating comparable model fit, with the logit model showing a very slightly lower AIC. Additionally, the McFadden’s pseudo R² for the logit model is slightly higher than that of the probit model, reinforcing the choice of the logit model as the preferred specification. [If we include the confusion matrix: From the confusion matrix, accuracy of the linear probability model, the probit, and logit models, all yield very similar results, however the logit model is ever so slightly higher.] While the differences between the two models are rather minor, the logit model consistently performs slightly better on both metrics.\n\n# average marginal effects \nlibrary(margins)\nmargins(swiss.logit)\n\nAverage marginal effects\n\n\nglm(formula = participation ~ income + age + education + youngkids +     oldkids + foreign, family = binomial(link = \"logit\"), data = SwissLabor)\n\n\n  income     age education youngkids   oldkids foreignyes\n -0.1699 -0.1064  0.006616   -0.2775 -0.004584     0.2834\n\n\nInterpretations of Average Marginal Effects: Income: There is an decrease in the probability of participating in the Swiss labor force, by about 17 percentage points, as the log of income-labor income increases. Age: There is a decrease in the probability of participation in the Swiss labor force by about 10.6 percentage points, as for each additional increase in decades. Education: there is a slight increase in the probability of participating in the Swiss labor force, by about 0.66 percentage points, for increases in years of education. Youngkids: There is a decrease in probability of participating in the Swiss labor force, by about 27.7 percentage points, for each additional young child. Oldkids: There is an extremely slight decrease, essentially negligiible, in the probability of participation in the Swiss labor force for each additional older child. Foreign: There is an increase in the probability of participating in the Swiss labor force, by about 28.3 percentage points, from people who aren’t foreigners to those who are.\nBased on the logit model selected as the final model, we computed the Average Marginal Effects (AMEs). The results indicate that higher non-labor income, older age, and more young children significantly reduce the probability of labor force participation, while being a foreigner substantially increases it by approximately 28.3 percentage points. Education exerts a small positive effect, increasing participation likelihood marginally per additional year of schooling. The number of older children has little meaningful impact on participation. Overall, the logit model effectively indentifies the key socioeconomic and demographic factors that influence labor force participation in Switzerland, confirming our hypothesis that variables such as income, age, education, and family responsibilities may play crucial roles."
  },
  {
    "objectID": "projects/02-project.html",
    "href": "projects/02-project.html",
    "title": "Economics 144 - AirPassengers",
    "section": "",
    "text": "# load data\nAirPassengers &lt;- AirPassengers"
  },
  {
    "objectID": "projects/02-project.html#a",
    "href": "projects/02-project.html#a",
    "title": "Economics 144 - AirPassengers",
    "section": "(a)",
    "text": "(a)\nPerform an additive decomposition of your series. Remove the trend and seasonality, and comment on the ACF and PACF of the residuals (i.e., what is left after detrending and seasonally adjusting the series). Comment on the results.\n\n# additive decomposition of the series\ndcmp_add_ap &lt;- decompose(ts(AirPassengers, start = c(1949, 1), freq = 12), \n                         \"additive\")\nplot(dcmp_add_ap, xlab = \"Year\")\n\n\n\n\n\n\n\npassengers &lt;- ts(AirPassengers, start = c(1949, 1), frequency = 12)\n\n# remove the trend and seasonality\n# trend component\nap_trend &lt;- dcmp_add_ap$trend\n# seasonal component\nap_seasonal &lt;- dcmp_add_ap$seasonal\n\n# Detrend and Seasonally Adjust: Y - T - S\ndetrend_seas_adj_ap &lt;- na.omit(passengers - ap_trend - ap_seasonal)\n\n# ACF and PACF plot\ntsdisplay(detrend_seas_adj_ap, main = \"ACF and PACF of Detrended & Seasonally \n          Adjusted AirPassengers\", xlab = \"Year\", \n          ylab = \"Residuals (Passengers)\")\n\n\n\n\n\n\n\n\nThe additive decomposition of the series is ideal when seasonal fluctuations remain relatively constant over time. However, after detrending and seasonally adjusting the AirPassengers data, the ACF and PACF plots reveal significant autocorrelation, indicating that the additive decomposition did not fully remove underlying patterns. Autocorrelation is evident in both ACF and PACF plots, as several residuals extend beyond the 95% confidence bounds. In particular, the PACF shows a strong spike at lag 1, suggesting a lack of stationarity and the persistence of trend or seasonal effects. The large autocorrelations observed in the ACF plot further imply dependence in the number of international airline passengers over time. Overall, these results suggest that a multiplicative decomposition or a transformation may provide a better representation of the data’s structure."
  },
  {
    "objectID": "projects/02-project.html#b",
    "href": "projects/02-project.html#b",
    "title": "Economics 144 - AirPassengers",
    "section": "(b)",
    "text": "(b)\nPerform a multiplicative decomposition of your series. Remove the trend and seasonality, and comment on the ACF and PACF of the residuals (i.e., what is left after detrending and seasonally adjusting the series). Comment on the results.\n\ndcmp_mult_ap &lt;- decompose(ts(AirPassengers, start = c(1949, 1), freq = 12), \n                          \"multiplicative\")\nplot(dcmp_mult_ap, xlab = \"Year\")\n\n\n\n\n\n\n\n# remove the trend and seasonality\n# trend component\nap_mult_trend &lt;- dcmp_mult_ap$trend\n# seasonal component\nap_mult_seasonal &lt;- dcmp_mult_ap$seasonal\n\n# Detrend and Seasonally Adjust: Y / T / S\ndetrend_seas_adj_mult_ap &lt;- na.omit((passengers / ap_mult_trend) / \n                                      ap_mult_seasonal)\n\ntsdisplay(detrend_seas_adj_mult_ap, main = \"ACF and PACF of Detrended & \n          Seasonally Adjusted AirPassengers\", xlab = \"Year\", \n          ylab = \"Residuals (Passengers)\")\n\n\n\n\n\n\n\n\nThe multiplicative decomposition of the series is most ideal when seasonal fluctuations vary in magnitude over time. After detrending and seasonally adjusting the AirPassengers data, the ACF and PACF plots reveal some remaining autocorrelation; however, the multiplicative decomposition aligns more closely with the underlying structure of the data than the additive model. Autocorrelation is still evident in both plots, as several residuals extend beyond the 95% confidence bounds. In particular, the PACF plot shows a larger spike at lag 1, suggesting a lack of stationarity and the persistence of trend or seasonal effects. The large autocorrelations observed in the ACF plot further indicate some dependence in the number of international airline passengers over time. Overall, while the multiplicative decomposition provides a better fit to the data’s structure than the additive approach, some systematic patterns remain in the residuals."
  },
  {
    "objectID": "projects/02-project.html#c",
    "href": "projects/02-project.html#c",
    "title": "Economics 144 - AirPassengers",
    "section": "(c)",
    "text": "(c)\nWhich decomposition is better, additive or multiplicative? Why?\nThe multiplicative decomposition provides a substantially better fit to the data than the additive decomposition. In the PACF plot of the multiplicative model, only a few larger spikes extend beyond the 95% confidence bounds, whereas the additive decomposition shows more frequent and pronounced lags. Furthermore, in the ACF plots, the autocorrelations from the multiplicative decomposition decay more rapidly than those from the additive model. Although some underlying patterns persist in the residuals of the multiplicative model, the overall dependence on the number of international airline passengers over time is weaker compared to the additive decomposition, indicating a better adjustment to the data’s structure."
  },
  {
    "objectID": "projects/02-project.html#d",
    "href": "projects/02-project.html#d",
    "title": "Economics 144 - AirPassengers",
    "section": "(d)",
    "text": "(d)\nBased on the two decompositions, and interpretation of the random components, would your models for the cycles be similar (additive vs. multiplicative) or very different? Why?\n\n# Additive Decomposition - Random Component\nap_random &lt;- dcmp_add_ap$random\nplot(ap_random, xlab = \"Year\", ylab = \"Residuals (Passengers)\", \n     main = \"Additive Decomposition: Random Component\")\n\n\n\n\n\n\n\n# Multiplicative Decomposition - Random Component\nap_mult_random &lt;- dcmp_mult_ap$random\nplot(ap_mult_random, xlab = \"Year\", ylab = \"Residuals\", \n     main = \"Multiplicative Decomposition: Random Component\")\n\n\n\n\n\n\n\n\nBased on the decomposition and the random components, the cycles in the additive and multiplicative models would be rather different. In the additive decomposition, the random component shows residuals that are larger at the beginning and end of the series but narrower in the middle, indicating non-constant variance over time. This pattern suggests that the additive model does not adequately account for the changing amplitude of the cyclical fluctuations as the number of passengers increases. In contrast, the multiplicative model produces residuals that are more evenly scattered over the years, appearing closer to random noise. This reflects that cyclical movements in passenger counts grow proportionally with the trend. Additionally, the multiplicative decomposition more accurately represents the cyclical structure of the AirPassengers data, where both the trend and the magnitude of seasonal and cyclical varianations increase over time."
  },
  {
    "objectID": "projects/02-project.html#e",
    "href": "projects/02-project.html#e",
    "title": "Economics 144 - AirPassengers",
    "section": "(e)",
    "text": "(e)\nPlot the seasonal factors and comment on the plot.\n\n# multiplicative, therefore, use log-transformation to stablize variance\nlpassengers &lt;- log(passengers)\n\n# fit the linear models\nfit1 &lt;- tslm(lpassengers ~ trend)\nfit2 &lt;- tslm(lpassengers ~ season)\nfit3 &lt;- tslm(lpassengers ~ trend + season)\n\n# plot the fitted models \nplot(lpassengers, main = \"Model 1: Time Series Data: Trend\", \n     ylab = \"Passengers\", xlab = \"Year\")\nlines(fit1$fitted.values, col = \"maroon4\", lwd = 2)\n\n\n\n\n\n\n\nplot(lpassengers, main = \"Model 2: Time Series Data: Seasonality\", \n     ylab = \"Passengers\", xlab = \"Year\")\nlines(fit2$fitted.values, col = \"maroon3\", lwd = 2)\n\n\n\n\n\n\n\nplot(lpassengers, main = \"Model 3: Time Series Data: Trend + Seasonality\", \n     ylab = \"Passengers\", xlab = \"Year\")\nlines(fit3$fitted.values, col = \"maroon2\", lwd = 2)\n\n\n\n\n\n\n\n# Compare the models with AIC and BIC tests\nAIC(fit1,fit2,fit3)\n\n     df       AIC\nfit1  3 -155.5887\nfit2 13  185.4405\nfit3 14 -390.5952\n\nBIC(fit1,fit2,fit3)\n\n     df       BIC\nfit1  3 -146.6793\nfit2 13  224.0480\nfit3 14 -349.0178\n\n# Plot the seasonal factors\nfit &lt;- tslm(lpassengers ~ season + 0)\nplot(fit$coef, type = \"l\", ylab = \"Seasonal Factors\", xlab = \"Month\", \n     main = \"Seasonal Factors for Air Passengers\", lwd = 2, col = \"dodgerblue2\")\n\n\n\n\n\n\n\n\nTo preface, we had taken the log-transformation of the passengers due to the multiplicative nature of the time series, in an attempt to stabilize the variance. The seasonal factor plot for the AirPassengers data show clear within-year patterns. Passenger numbers are lowest in the early months (January-February), rise steadily through the spring, and peak during the summer months (July-August), with a smaller increase in December. This is consistent with seasonal travel behavior as people tend travel more due to factors such as warmer weather, school breaks and holidays, and the holiday season. The fitted models indicate that including both trend and seasonality provides the best representation to the data, as confirmed by the results of the AIC and BIC tests (having the lowest values for the trend + seasonality model)."
  },
  {
    "objectID": "projects/02-project.html#f",
    "href": "projects/02-project.html#f",
    "title": "Economics 144 - AirPassengers",
    "section": "(f)",
    "text": "(f)\nBased on your analysis thus far, choose a model that includes your preferred trend and seasonal model to forecast 12-steps ahead, and show the plot of the data, respective fit, and forecast.\n\n# the data is multiplicative (variance grows with the trend)\n# log-transformed trend and seasonality model \nlpassengers &lt;- log(passengers)\n\n# fit the preferred linear trend \nfit_pref &lt;- tslm(lpassengers ~ trend + season)\nplot(forecast(fit_pref, h = 12), main = \"Forecast - Trend + Seasonality\", \n     xlab = \"Year\", ylab = \"log(Passengers)\")\nlines(fit_pref$fitted.values, col = \"maroon1\", lwd = 2)\n\nlegend(\"topleft\", legend = c(\"Observed Data\", \"Model Fit\", \"Forecast\"), \n       col = c(\"black\", \"maroon1\", \"blue\"), lwd = c(2, 3, 3), bty = \"n\")\n\n\n\n\n\n\n\n# improve the forecast using ets\nfit_ets &lt;- ets(passengers)\nplot(fit_ets)\n\n\n\n\n\n\n\naccuracy(fit_ets)\n\n                   ME     RMSE      MAE       MPE     MAPE      MASE       ACF1\nTraining set 1.567359 10.74726 7.791605 0.4357799 2.857917 0.2432573 0.03945056\n\nplot(forecast(fit_ets, level = c(50, 80, 95), h = 12))\n\n\n\n\n\n\n\n\nBased on the analysis thus far, the preferred model for forecasting the AirPassengers data is the log-transformed linear trend + seasonal model, which accounts for the multiplicative nature of the data. The logarithmic transformation stabilizes the variance and converts the multiplicative relationship between trend and seasonality into an additive one, making the model more suitable for linear regression analysis. The model captures the observed time series well and our forecast for the next 12 months, illustrated with a continued upwards trend alongside recurring seasonal peaks. To improve the accuracy of the forecast, an ETS model was also fitted to the original data. The ETS forecast produces similar results to the regression model but automatically adjusts for changing seasonal amplitudes, providing a more flexible and robust forecast of future passenger numbers on international airlines."
  },
  {
    "objectID": "projects/01-project.html",
    "href": "projects/01-project.html",
    "title": "Economics 144 - US Employment Models",
    "section": "",
    "text": "I. Introduction\nIn this project, we analyze and forecast US employment using the “us_employment” dataset from the ffp3 package in RStudio. The dataset contains monthly employment data from January 1939 to June 2019. Our focus is on the series “All Employees, Total Nonfarm,” which captures total US employment excluding farmworkers. We split the data into training and test sets to evaluate several forecasting methods and compare their predictions against the actual test data. Specifically, we apply ARIMA, ETS, Holt-Winters, NNETAR, and Prophet models, and also create a combined forecast to assess whether averaging multiple forecasts improves accuracy. Model performance is evaluated using accuracy metrics, residual diagnostics, and autocorrelation tests, allowing us to identify the model that best captures the patterns in total nonfarm employment and provides the most reliable forecasts.\ntotal_nonfarm &lt;- us_employment %&gt;%\n  filter(Title == \"All Employees, Total Nonfarm\")\nemployment_ts &lt;- ts(total_nonfarm$Employed, start=1939, frequency=12)\nplot(employment_ts, xlab=\"Year\", ylab=\"Total Employment\", main=\"US Total Employment (Nonfarm)\")\nThe time series plot for total employment in the US (nonfarming) shows an increasing trend from 1939 to 2019. There are some small dips in the number of employed throughout the years, but it does look like the general total employment increases. Seasonality in the data is also visible, and especially so past 1960 where the fluctuations indicating seasonality become more regular and continue on until the last observations. Some cycles may also be present in the data as the rises and dips in employment come in groups every so often. For example, the cycling can be seen around 1950, 1980, 2000, and 2008.\ntsdisplay(employment_ts)\nThe tsdisplay showcasing the ACF and PACF plot of the total employment data shows a very prolonged decay of the spikes in the ACF plot, suggesting high persistence because of how slow the spikes take to get down to a magnitude of 0. The PACF plot shows one very significant spike out of the bound windows, with 3 very slightly significant spikes further along the lags that could suggest some seasonal component could be taken into account for a model.\nemployment_ts.train &lt;- window(employment_ts, end=c(2017,6))\nemployment_ts.test &lt;- window(employment_ts, start=c(2017,7))\n\nautoplot(employment_ts) +\n  autolayer(employment_ts.train, series=\"Training\") +\n  autolayer(employment_ts.test, series=\"Test\") + theme_minimal()"
  },
  {
    "objectID": "projects/01-project.html#arima-forecast",
    "href": "projects/01-project.html#arima-forecast",
    "title": "Economics 144 - US Employment Models",
    "section": "ARIMA Forecast",
    "text": "ARIMA Forecast\n\nemployment_arima &lt;- auto.arima(employment_ts.train)\nemployment_arima\n\nSeries: employment_ts.train \nARIMA(2,0,1)(0,1,1)[12] with drift \n\nCoefficients:\n         ar1      ar2      ma1     sma1     drift\n      1.8946  -0.9000  -0.6119  -0.6396  123.1009\ns.e.  0.0210   0.0209   0.0376   0.0281   15.8391\n\nsigma^2 = 49198:  log likelihood = -6346.03\nAIC=12704.06   AICc=12704.15   BIC=12733.07\n\n\nFor the ARIMA model for total (nonfarm) employment in the US, the auto.arima output suggests that an ARIMA(2,0,1)(0,1,1) process would fit the data. This result suggests an AR(2), MA(1) process with 1 seasonal differencing, and seasonal MA(1). It takes into account the increasing trend of the train set for employment, as well as the seasonality that is visible in the time series plot of the original data.\n\narima_forecast &lt;- forecast(employment_arima, h=27)\nautoplot(arima_forecast, xlim=c(2000,2020), ylim=c(120000,160000)) +\n  autolayer(employment_ts.test, series = \"Test\") + theme_minimal()\n\nScale for x is already present.\nAdding another scale for x, which will replace the existing scale.\n\n\nWarning: Removed 732 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nThe forecast using the ARIMA model fit looks to follow along well with the pattern and fluctuations of the actual observed data, especially in the first few steps ahead. However, after those first few steps, the ARIMA forecast does not seem to be able to keep up with the increasing trend that is visible in the observed data. The forecast does follow along with the general trend of increasing employment as the years increase, but is not able to match the magnitude of increase that has actually been recorded."
  },
  {
    "objectID": "projects/01-project.html#ets-forecast",
    "href": "projects/01-project.html#ets-forecast",
    "title": "Economics 144 - US Employment Models",
    "section": "ETS Forecast",
    "text": "ETS Forecast\n\nemployment_ets &lt;- ets(employment_ts.train)\n\nets_forecast &lt;- forecast(employment_ets, h=27)\nautoplot(ets_forecast, xlim=c(2000,2020), ylim=c(120000,160000)) + \n  autolayer(employment_ts.test) + theme_minimal()\n\nScale for x is already present.\nAdding another scale for x, which will replace the existing scale.\n\n\nWarning: Removed 732 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nThe forecast for total US employment using the ETS method is similar to the ARIMA model where the general shape follows closely with the actual observed data. This method also has the same issue where it is unable to predict a higher magnitude of increase to the number of employed people in the US. The forecast here predicts lower employment numbers compared to the ARIMA model forecast. The observed data sits in the upper bounds of the confidence interval for this method."
  },
  {
    "objectID": "projects/01-project.html#compare-training-testing-errors",
    "href": "projects/01-project.html#compare-training-testing-errors",
    "title": "Economics 144 - US Employment Models",
    "section": "Compare Training & Testing Errors",
    "text": "Compare Training & Testing Errors\n\nacc &lt;- tibble(\n  model = c(\"ARIMA\", \"ETS\"),\n  RMSE  = c(\n    accuracy(arima_forecast, employment_ts.test)[\"Test set\", \"RMSE\"],\n    accuracy(ets_forecast,   employment_ts.test)[\"Test set\", \"RMSE\"]),\n  MAE = c(\n    accuracy(arima_forecast, employment_ts.test)[\"Test set\", \"MAE\"],\n    accuracy(ets_forecast,   employment_ts.test)[\"Test set\", \"MAE\"]),\n  MAPE = c(\n    accuracy(arima_forecast, employment_ts.test)[\"Test set\", \"MAPE\"],\n    accuracy(ets_forecast,   employment_ts.test)[\"Test set\", \"MAPE\"])\n)\nacc\n\n# A tibble: 2 × 4\n  model  RMSE   MAE  MAPE\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 ARIMA 1118.  896. 0.596\n2 ETS   1778. 1507. 1.00 \n\n\n\nemployment_tsibble &lt;- as_tsibble(employment_ts)\n\n# Train/test split\ntrain &lt;- employment_tsibble |&gt; filter_index(~ \"2017-06\")\ntest  &lt;- employment_tsibble |&gt; filter_index(\"2017-07\" ~ .)\n\n# Fit models\nfit &lt;- train |&gt;\n  model(\n    arima   = ARIMA(value),\n    ets     = ETS(value)\n  )\n\n\n# Forecast horizon\nh &lt;- nrow(test)\nfc &lt;- fit |&gt; forecast(h = h)\n\n# Plot\nfc |&gt; autoplot(employment_tsibble) +\n  coord_cartesian(xlim=c(yearmonth(\"2000 Jan\"), yearmonth(\"2020 Jan\")), ylim = c(120000, 160000))\n\n\n\n\n\n\n\n# Accuracy\nfc |&gt; accuracy(test)\n\n# A tibble: 2 × 10\n  .model .type    ME  RMSE   MAE   MPE  MAPE  MASE RMSSE  ACF1\n  &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 arima  Test   861. 1118.  896. 0.572 0.596   NaN   NaN 0.904\n2 ets    Test  1507. 1778. 1507. 1.00  1.00    NaN   NaN 0.892"
  },
  {
    "objectID": "projects/01-project.html#holt-winters",
    "href": "projects/01-project.html#holt-winters",
    "title": "Economics 144 - US Employment Models",
    "section": "Holt-Winters,",
    "text": "Holt-Winters,\n\nh &lt;- length(employment_ts.test)\n\nhw_add &lt;- hw(employment_ts.train, seasonal = \"additive\", h = h)\nhw_mul &lt;- hw(employment_ts.train, seasonal = \"multiplicative\", h = h)\n\nautoplot(employment_ts, series = \"Observed\") +\n  autolayer(hw_mul, series = \"HW Multiplicative\") +\n  autolayer(hw_add, series = \"HW Additive\") +\n  autolayer(employment_ts.test, series = \"Test (Actual)\") +\n  labs(\n    title = \"Holt–Winters Forecasts (Additive vs Multiplicative)\",\n    x = \"Year\",\n    y = \"Total Employment (Nonfarm)\"\n  ) +\n  guides(colour = guide_legend(title = \"Series\")) +\n  coord_cartesian(xlim = c(2000, 2020)) + theme_minimal()\n\n\n\n\n\n\n\nacc_hw_add &lt;- accuracy(hw_add, employment_ts.test)[\"Test set\", c(\"RMSE\",\"MAE\",\"MAPE\")]\nacc_hw_mul &lt;- accuracy(hw_mul, employment_ts.test)[\"Test set\", c(\"RMSE\",\"MAE\",\"MAPE\")]\n\nacc_summary &lt;- tibble(\n  model = c(\"HW_additive\", \"HW_multiplicative\"),\n  RMSE = c(acc_hw_add[\"RMSE\"], acc_hw_mul[\"RMSE\"]),\n  MAE  = c(acc_hw_add[\"MAE\"],  acc_hw_mul[\"MAE\"]),\n  MAPE = c(acc_hw_add[\"MAPE\"], acc_hw_mul[\"MAPE\"])\n)\n\nacc_summary\n\n# A tibble: 2 × 4\n  model              RMSE   MAE  MAPE\n  &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 HW_additive        259.  212. 0.141\n2 HW_multiplicative  526.  453. 0.302\n\n\nThe additive model yields much lower RMSE, MAE, and MAPE (258.7, 211.6, 0.141) than the multiplicative model (525.8, 452.8, 0.302). Accordingly, we adopt the additive Holt–Winters specification for subsequent analysis.\n\npreferred_hw &lt;- hw_add\nresid_hw &lt;- residuals(preferred_hw)\n\nggtsdisplay(resid_hw, main = \"Residual Diagnostics — Holt–Winters Additive\")\n\n\n\n\n\n\n\nBox.test(resid_hw, lag = 24, type = \"Ljung-Box\", fitdf = 0)\n\n\n    Box-Ljung test\n\ndata:  resid_hw\nX-squared = 576.56, df = 24, p-value &lt; 2.2e-16\n\n\nThe residuals show clear serial correlation, with long runs of positive and negative errors and several persistent shocks. Additionally, the Ljung–Box test strongly rejects white noise (p &lt; 2.2e-16), further confirming that substantial autocorrelation remains. Thus, while Holt–Winters captures the trend and seasonality, it does not fully model the underlying dynamics, and a more flexible model would be required for a proper statistical fit."
  },
  {
    "objectID": "projects/01-project.html#nnetar",
    "href": "projects/01-project.html#nnetar",
    "title": "Economics 144 - US Employment Models",
    "section": "NNETAR",
    "text": "NNETAR\n\nset.seed(123)\n\nnnet_model &lt;- nnetar(employment_ts.train)\n\nnnet_fc &lt;- forecast(nnet_model, h = length(employment_ts.test))\n\nautoplot(nnet_fc) +\n  autolayer(employment_ts.test, series = \"Test\") +\n  labs(title = \"NNETAR Forecast\",\n       x = \"Year\",\n       y = \"Total Employment\") + theme_minimal()\n\n\n\n\n\n\n\nnnet_accuracy &lt;- accuracy(nnet_fc, employment_ts.test)\nnnet_model\n\nSeries: employment_ts.train \nModel:  NNAR(1,1,2)[12] \nCall:   nnetar(y = employment_ts.train)\n\nAverage of 20 networks, each of which is\na 2-2-1 network with 9 weights\noptions were - linear output units \n\nsigma^2 estimated as 607219\n\nnnet_accuracy\n\n                       ME      RMSE       MAE         MPE      MAPE      MASE\nTraining set 2.232815e-02  779.2424  571.9611 -0.01166318 0.7053657 0.2770047\nTest set     2.698867e+03 3203.5652 2798.9652  1.79349778 1.8621064 1.3555581\n                  ACF1 Theil's U\nTraining set 0.2166367        NA\nTest set     0.7130423  2.921429\n\n\nThe plot shows that the NNETAR model’s mean forecast becomes noticeably flattened and smooth, failing to reproduce the sharp swings that appear in the actual data. Additionally, the test-set errors are much larger than the training errors (RMSE = 3204, MAPE = 1.86%), showing that the model does not generalize well to data beyond the training set.\n\ntsdisplay(residuals(nnet_model), main = \"Residuals from NNETAR model\")\n\n\n\n\n\n\n\nBox.test(residuals(nnet_model), lag = 24, type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  residuals(nnet_model)\nX-squared = 2069.1, df = 24, p-value &lt; 2.2e-16\n\n\nThe residuals from the NNETAR model exhibit noticeable remaining structure. In particular, the time-series plot shows recurring negative spikes consistent with seasonal patterns that the network did not fully capture. Additionally, the ACF and PACF plots display several statistically significant autocorrelations across a wide range of lags, indicating that the residuals are not behaving like white noise. Consistent with this, the Box–Ljung test strongly rejects the null of no autocorrelation (p &lt; 2.2e−16). Together, these diagnostics suggest that the NNETAR model has not fully extracted all temporal dependence in the series, leaving meaningful autocorrelation in the residuals, which implies that the network would need a richer structure such as additional lag inputs, more hidden units, or a larger seasonal embedding to adequately learn the underlying dynamics of total nonfarm employment in the US."
  },
  {
    "objectID": "projects/03-project.html",
    "href": "projects/03-project.html",
    "title": "Economics 104 - Natural Gas Panel Dataset",
    "section": "",
    "text": "data(\"NaturalGas\")\n\nnatural_gas &lt;- subset(NaturalGas, state == \"NY\" |\n                        state == \"FL\" | state == \"MI\" | state == \"TX\" | state == \"CA\")\n\nnatural_gas_pd &lt;- pdata.frame(natural_gas, index = c(\"state\", \"year\"))\nclass(natural_gas_pd) # make sure it's a panel data\n\n[1] \"pdata.frame\" \"data.frame\" \n\ndim(natural_gas_pd) # dimension\n\n[1] 115  10\n\nis.pbalanced(natural_gas_pd) # is it balanced\n\n[1] TRUE\n\n\n\na. Data Discussion\nBriefly discuss the question you are trying to answer.\nFor this panel data projet, we’re using the Natural Gas dataset from the AERpackage in R, which contains 115 observations from 5 US states over the period 1967-1989. The source of the data is from Baltagi (2002). The dataset includes variables such as consumption (consumption of natural gas by the residential sector), price (the price of natural gas), eprice, (price of electricity), oprice (price of distillate fuel oil), lprice (price of liquefied petroleum gas), heating (heating degree days), and income (real per-capita personal income). Specifically, the question we are trying to answer is: How do prices (of natural gas, electricity, distillate fuel oil, and liquefied petroleum gas),heating degree days, and personal income affect consumption of natural gas? The dimensions of the panel dataset is 115 rows and 10 columns and is therefore long and narrow. The panel dataset is balanced.\nThe citation of the dataset is: Baltagi, B.H. (2002). Econometrics, 3rd ed. Berlin, Springer.\n\n\nb. Variable Description\nProvide a descriptive analysis of your variables. This should include relevant figures with comments including some graphical depiction of individual hetero-geneity; no unnecessary figures!\n\n# histograms for each variable\npar(mfrow = c(2,2))\nhist(natural_gas_pd$consumption, main = \"Histogram of Consumption\")\nhist(natural_gas_pd$price, main = \"Histogram of Price\")\nhist(natural_gas_pd$eprice, main = \"Histogram of ePrice\")\nhist(natural_gas_pd$oprice, main = \"Histogram of oPrice\")\n\n\n\n\n\n\n\npar(mfrow = c(2,2))\nhist(natural_gas_pd$lprice, main = \"Histogram of lPrice\")\nhist(natural_gas_pd$heating, main = \"Histogram of Heating\")\nhist(natural_gas_pd$income, main = \"Histogram of Income\")\n\n\n\n\n\n\n\n\nI ran histograms for each feature and the response in the case of an extreme right skew. In which case, we would implement the log of the variable to better adhere to the linear regression assumptions. From the plots we see that while there are some moderate right skews, none of which are rather extreme, and therefore, we will not need to log transform the respective variables.\nConsumption: Residential gas consumption is somewhat normal and however, has a second peak towards the left tail. The two peaks are between 0 and 50,000 and 300,000 and 350,000, suggesting that these are perhaps the more common ranges for annual residential gas consumption across states. The range of the data is between 0 to 65,000 with limited to no outliers observed.\nPrice: The price of natural gas is slightly right-skewed, indicating that while the price of natural gas has been relatively low across most state-years, there have been a few instances of higher prices. The peak occurs around 1 to 2, suggesting that this is the common range of prices for natural gas across states. The range of prices is from 0 to 9 with limited to no outliers observed.\nePrice: The price of electricity is slightly right-skewed, indicating that while the price of electricity has been relatively lower across most state-years, there have been a few instances of higher electricity prices. The peak occurs around 2 to 3, suggesting that this is the common range of prices for electricity across states. The range of prices for electricity is from 1 to 11 with limited to no outliers observed.\noPrice: The price of distillate fuel oil is very slightly right-skewed, indicating that while the price of distillate fuel oil has been somewhat relatively lower across most state-years, there have been fewer years of higher distillate fuel oil prices as well. The peak occurs around 5 to 10, suggesting that this is the most common range of prices for distillate fuel oil. The range of prices is from 5 to 55 with limited to no outliers observed.\nlPrice: The price of liquefied petroleum gas has a very slight right-skew, indicating that while the price of liquefied petroleum gas has been somewhat relatively lower across most state-years, there have been fewer years of higher liquefied petroleum gas as well. The peak occurs around 1 to 2, suggesting that this is the common range of prices for liquefied petroleum gas. The range of prices is from 0 to 8 with limited to no outliers observed.\nHeating: Heating degree days looks to have a bi-modal distribution with two peaks from 2000 to 3000, and 6000 to 7000, suggesting that these are perhaps the more common ranges for heating degree days across states. The range of data is between 0 to 8000 with limited to no outliers observed.\nIncome: Income across all states and years is approximately normally distributed, indicating that most state-year observations cluster around a central average, with fewer very low or very high values. There is no strong skew, and extreme outliers seem limited.\n\nscatterplot(consumption ~ state | year, data = natural_gas_pd)\n\n\n\n\n\n\n\n\n[1] \"24\"  \"35\"  \"57\"  \"60\"  \"104\" \"105\"\n\n\nFrom the scatterplot of consumption across states, we see a varying range of the data, with California having the largest range and Florida seemingly have the shortest. Additionally, the means with respect to the level of consumption indicates rather varying level of consumption per state. Therefore, both the average level of consumption and the range of consumption seem different across states, suggesting individual heterogeneity.\n\nscatterplot(price ~ state | year, data = natural_gas_pd)\n\n\n\n\n\n\n\n\nFrom the scatterplot of the price of natural gas across states, we see a similar price range of the data, and somewhat similar average price levels as well. Based on the similar price levels and ranges across states, this scatterplot therefore does not suggest individual heterogeneity, though further testing is necessary.\n\nscatterplot(eprice ~ state | year, data = natural_gas_pd)\n\n\n\n\n\n\n\n\nFrom the scatterplot of the price of electricity across states, we see a slightly varying price range of the data, and however, somewhat similar average price levels. Based on the similar price levels and slightly varying ranges across states, this scatterplot does not suggest individual heterogeneity, though further testing is necessary.\n\nscatterplot(oprice ~ state | year, data = natural_gas_pd)\n\n\n\n\n\n\n\n\nFrom the scatterplot of the price of distillate fuel oil across states, we see a similar price range of the data, and similar average price levels. Based on the similar price levels and ranges across states, this scatterplot does notsuggest individual heterogeneity.\n\nscatterplot(lprice ~ state | year, data = natural_gas_pd)\n\n\n\n\n\n\n\n\nFrom the scatterplot of the price of liquefied petroleum gas, we see a similar price range of the data, and somewhat varying average price levels. Based on the similar price levels and slightly varying ranges across states, this scatterplot therefore does not suggest individual heterogeneity, however further testing is necessay.\n\nscatterplot(heating ~ state | year, data = natural_gas_pd)\n\n\n\n\n\n\n\n\n[1] \"53\" \"67\"\n\n\nFrom the scatterplot of heating degree days, we see a similar range of the data across states. Additionally, the means with respect to the level of heating indicates an extremely varying level of heating degree days per state. Therefore, while the range of the level of heating is rather similar across states, the average level greatly varies, suggesting indivdual heterogeneity, though further testing is necessary.\n\nscatterplot(income ~ state | year, data = natural_gas_pd)\n\n\n\n\n\n\n\n\n[1] \"91\" \"92\"\n\n\nFrom the scatterplot of per-capita personal income, we see varying ranges of the data across states. Additionally, the means with respect to the level of income indicates an moderate level of variation of income per state. Therefore, while the range and price of the level of income moderately vary, individual heterogeneity is suggested, though further testing is necessary.\nBased on the analysis of the observable variables, it is suggested that individual heterogeneity may be present in the model, alluding to the fact the differences may be attributed to factors or variables that are unobserved or unaccounted for.\n\nplotmeans(consumption ~ state, data = natural_gas_pd)\n\nWarning in arrows(x, li, x, pmax(y - gap, li), col = barcol, lwd = lwd, :\nzero-length arrow is of indeterminate angle and so skipped\nWarning in arrows(x, li, x, pmax(y - gap, li), col = barcol, lwd = lwd, :\nzero-length arrow is of indeterminate angle and so skipped\nWarning in arrows(x, li, x, pmax(y - gap, li), col = barcol, lwd = lwd, :\nzero-length arrow is of indeterminate angle and so skipped\nWarning in arrows(x, li, x, pmax(y - gap, li), col = barcol, lwd = lwd, :\nzero-length arrow is of indeterminate angle and so skipped\nWarning in arrows(x, li, x, pmax(y - gap, li), col = barcol, lwd = lwd, :\nzero-length arrow is of indeterminate angle and so skipped\n\n\nWarning in arrows(x, ui, x, pmin(y + gap, ui), col = barcol, lwd = lwd, :\nzero-length arrow is of indeterminate angle and so skipped\nWarning in arrows(x, ui, x, pmin(y + gap, ui), col = barcol, lwd = lwd, :\nzero-length arrow is of indeterminate angle and so skipped\nWarning in arrows(x, ui, x, pmin(y + gap, ui), col = barcol, lwd = lwd, :\nzero-length arrow is of indeterminate angle and so skipped\nWarning in arrows(x, ui, x, pmin(y + gap, ui), col = barcol, lwd = lwd, :\nzero-length arrow is of indeterminate angle and so skipped\nWarning in arrows(x, ui, x, pmin(y + gap, ui), col = barcol, lwd = lwd, :\nzero-length arrow is of indeterminate angle and so skipped\n\n\n\n\n\n\n\n\n\nThe above plot indicates that across each unit (states), there are differences in the average level of consumption of natural gas.\n\nplotmeans(consumption ~ year, data = natural_gas_pd)\n\n\n\n\n\n\n\n\nThe above plot suggests that overtime, the average level of consumption doesn’t seem to fluctuate much. Additionally, the range also seems to be of a constant size over time. From the plot, statistical insignificance of the differnce maybe be suggested but further testing is necessary to make such a conclusion. Overall, while there may be consistency across the years, there seems to be significant individual differences across states.\n\n\nc. Model Fitting\nc.Fit the three models below, and identify which model is your preferred one and why. Make sure to include your statistical diagnostics to support your conclusion, and to comment on your findings.\n\npooledreg1 &lt;- plm(consumption~price+eprice+oprice+lprice+heating+income,\n                  model=\"pooling\",data=natural_gas_pd)\n\n# Cluster robust standard errors\ncrse&lt;- coeftest(pooledreg1, vcov=vcovHC(pooledreg1,\ntype=\"HC0\",cluster=\"group\"))\n\nstargazer(pooledreg1, crse, column.labels = c(\"\\\\textit{Pooled}\", \n                                              \"\\\\textit{Pooled(prse)}\"),\n        model.names = FALSE,type = \"text\")\n\n\n====================================================\n                       Dependent variable:          \n             ---------------------------------------\n                   consumption                      \n                     Pooled           Pooled(prse)  \n                       (1)                 (2)      \n----------------------------------------------------\nprice            -104,163.000***     -104,163.000***\n                  (34,055.000)        (21,477.000)  \n                                                    \neprice             27,709.000*         27,709.000   \n                  (15,928.000)        (28,194.000)  \n                                                    \noprice             3,868.000**         3,868.000**  \n                   (1,550.000)         (1,841.000)  \n                                                    \nlprice             -15,593.000         -15,593.000  \n                  (29,488.000)        (16,149.000)  \n                                                    \nheating               5.112               5.112     \n                     (4.627)             (7.634)    \n                                                    \nincome              85.770***           85.770***   \n                     (9.856)            (27.520)    \n                                                    \nConstant         -538,137.000***     -538,137.000** \n                  (89,926.000)        (207,779.000) \n                                                    \n----------------------------------------------------\nObservations           115                          \nR2                    0.727                         \nAdjusted R2           0.711                         \nF Statistic  47.830*** (df = 6; 108)                \n====================================================\nNote:                    *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nThe coefficients from the pooled model may be interpretted as such: A one-unit increase in the price of natural gas is associated with a 104,163 unit decrease in natural gas consumption, on average, across all states and years. A one-unit increase in the price of electricity is associated with a 27,709 unit increase in electricity consumption, on average, across all states and years. A one-unit increase in the price of distillate fuel oil is associated with a 3,868 unit increase in distillate fuel oil consumption, on average, across all states and years. A one-unit increase in the price of liquefied petroleum gas is associated with a 15,593 unit decrease in distillate fuel oil consumption, on average, across all states and years. A one-unit increase in the heating degree days is associated with a 5.112 unit increase in consumption, on average, across all states and years. A one-unit increase in the per-capita personal income is associated with a 85.77 unit increase in consumption, on average, across all states and years.\n\n# Fixed effect model: one way (individual)\nwithinreg1 &lt;- plm(consumption~price+eprice+oprice+lprice+heating+income, model=\"within\", effect = \"individual\", data=natural_gas_pd)\nsummary(withinreg1)\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = consumption ~ price + eprice + oprice + lprice + \n    heating + income, data = natural_gas_pd, effect = \"individual\", \n    model = \"within\")\n\nBalanced Panel: n = 5, T = 23, N = 115\n\nResiduals:\n   Min. 1st Qu.  Median 3rd Qu.    Max. \n -66534  -12662     214   11717   80863 \n\nCoefficients:\n        Estimate Std. Error t-value Pr(&gt;|t|)    \nprice    8550.60    9935.04    0.86    0.391    \neprice  -7366.93    5722.26   -1.29    0.201    \noprice     75.25     490.75    0.15    0.878    \nlprice  -7267.64    8547.89   -0.85    0.397    \nheating    46.93      10.45    4.49 0.000018 ***\nincome      6.75       3.96    1.71    0.091 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    8.89e+10\nResidual Sum of Squares: 6.63e+10\nR-Squared:      0.254\nAdj. R-Squared: 0.183\nF-statistic: 5.91678 on 6 and 104 DF, p-value: 0.0000241\n\n\nThis model accounts for individual effects. We may conclude that generally (and the term generally may be applied to all other coefficient interpretations for this set of comments) there is 8550.60 increase in the level of consumption when the price of natural gas increases by 1 unit. There is 7366.93 decrease in the level of consumption when the price of electricity increases by one unit. There is a 75.25 increase in the level of consumption when the price of distillate fuel oil increases by one unit. There is a 7267.64 decrease in the level of consumption when the price of liquefied petroleum gas increases by one unit. There is a 46.93 increase in the level of consumption when heating degree days increaes by one unit. And there is a 6.75 increase in the level of consumption when per-capita personal income increases by one unit. This is the result within each state once accounting for the differences between states.\n\n# Fixed effect model: one way (time)\nwithinreg2 &lt;- plm(consumption~price+eprice+oprice+lprice+heating+income, model=\"within\", effect = \"time\", data=natural_gas_pd)\nsummary(withinreg2)\n\nOneway (time) effect Within Model\n\nCall:\nplm(formula = consumption ~ price + eprice + oprice + lprice + \n    heating + income, data = natural_gas_pd, effect = \"time\", \n    model = \"within\")\n\nBalanced Panel: n = 5, T = 23, N = 115\n\nResiduals:\n   Min. 1st Qu.  Median 3rd Qu.    Max. \n-114033  -43894   -1522   50755  163319 \n\nCoefficients:\n         Estimate Std. Error t-value Pr(&gt;|t|)    \nprice   -7.24e+04   3.67e+04   -1.97   0.0517 .  \neprice   3.68e+04   1.37e+04    2.68   0.0088 ** \noprice   2.89e+03   3.21e+03    0.90   0.3719    \nlprice  -1.08e+05   3.79e+04   -2.84   0.0056 ** \nheating  4.02e-01   3.95e+00    0.10   0.9190    \nincome   9.00e+01   9.59e+00    9.38  8.1e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    3.5e+12\nResidual Sum of Squares: 4.99e+11\nR-Squared:      0.857\nAdj. R-Squared: 0.811\nF-statistic: 86.0979 on 6 and 86 DF, p-value: &lt;2e-16\n\n\nThis model accounts for time effects. We may conclude that generally (and the term generally may be applied to all other coefficient interpretations for this set of comments) there is 7.24e+04 decrease in the level of consumption when the price of natural gas increases by 1 unit. There is 3.68e+04 increase in the level of consumption when the price of electricity increases by one unit. There is a 2.89e+03 increase in the level of consumption when the price of distillate fuel oil increases by one unit. There is a 1.08e+05 decrease in the level of consumption when the price of liquefied petroleum gas increases by one unit. There is a 4.02e-01 increase in the level of consumption when heating degree days increase by one unit. And there is a 9.00e+01 increase in the level of consumption when per-capita personal income increases by one unit. This is the result within each year once accounting for the differences across time.\n\n# Fixed effect model: twoways\nwithinreg3 &lt;- plm(consumption~price+eprice+oprice+lprice+heating+income, model=\"within\", effect = \"twoways\", data=natural_gas_pd)\nsummary(withinreg3)\n\nTwoways effects Within Model\n\nCall:\nplm(formula = consumption ~ price + eprice + oprice + lprice + \n    heating + income, data = natural_gas_pd, effect = \"twoways\", \n    model = \"within\")\n\nBalanced Panel: n = 5, T = 23, N = 115\n\nResiduals:\n   Min. 1st Qu.  Median 3rd Qu.    Max. \n -60442  -11674    -136   12153   56993 \n\nCoefficients:\n        Estimate Std. Error t-value Pr(&gt;|t|)    \nprice   12557.58   11579.83    1.08     0.28    \neprice  -8425.45    5926.83   -1.42     0.16    \noprice    803.70    1000.90    0.80     0.42    \nlprice  12545.43   12460.36    1.01     0.32    \nheating    60.55      11.03    5.49  4.4e-07 ***\nincome     -2.03       5.67   -0.36     0.72    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    6.12e+10\nResidual Sum of Squares: 4.16e+10\nR-Squared:      0.32\nAdj. R-Squared: 0.0544\nF-statistic: 6.42587 on 6 and 82 DF, p-value: 0.000014\n\n\nThis model accounts for two-ways (individual and time) effects. We may conclude that generally (and the term generally may be applied to all other coefficient interpretations for this set of comments) there is 12557.58 increase in the level of consumption when the price of natural gas increases by 1 unit. There is 8425.45 decrease in the level of consumption when the price of electricity increases by one unit. There is a 803.7 increase in the level of consumption when the price of distillate fuel oil increases by one unit. There is a 12545.43 increase in the level of consumption when the price of liquefied petroleum gas increases by one unit. There is a 60.55 increase in the level of consumption when heating degree days increase by one unit. And there is a 2.03 decrease in the level of consumption when per-capita personal income increases by one unit.\n\npFtest(withinreg1, pooledreg1) \n\n\n    F test for individual effects\n\ndata:  consumption ~ price + eprice + oprice + lprice + heating + income\nF = 352, df1 = 4, df2 = 104, p-value &lt;2e-16\nalternative hypothesis: significant effects\n\n# we reject the null so there seems like there are individual effects\n\nWe have implemented the F-test for individual effects. The null hypothesis is that there are no individual effects and the alternate hypothesis states that there are effects and we must use a model that accounts for these differences across individuals. We reject the null, indicating that there may be individual effects.\n\npFtest(withinreg2, pooledreg1)\n\n\n    F test for time effects\n\ndata:  consumption ~ price + eprice + oprice + lprice + heating + income\nF = 3.6, df1 = 22, df2 = 86, p-value = 9e-06\nalternative hypothesis: significant effects\n\n# we reject the null so there seems to be time effects as well.\n\nWe have implemented the F-test for time effects. From the p-value, we reject the null hypothesis that there are no time effects and conclude that there may be time effects and we must use a model that accounts for these differences across years.\n\n# Random effects model\nrandom_effects &lt;- plm(consumption ~ price + eprice  + oprice + lprice + heating + income, data=natural_gas_pd, model=\"random\", random.method = \"amemiya\")\n\nsummary(random_effects)\n\nOneway (individual) effect Random Effect Model \n   (Amemiya's transformation)\n\nCall:\nplm(formula = consumption ~ price + eprice + oprice + lprice + \n    heating + income, data = natural_gas_pd, model = \"random\", \n    random.method = \"amemiya\")\n\nBalanced Panel: n = 5, T = 23, N = 115\n\nEffects:\n                   var  std.dev share\nidiosyncratic 6.02e+08 2.45e+04  0.02\nindividual    2.41e+10 1.55e+05  0.98\ntheta: 0.967\n\nResiduals:\n   Min. 1st Qu.  Median 3rd Qu.    Max. \n -56760  -14490   -1843    9973   90718 \n\nCoefficients:\n            Estimate Std. Error z-value Pr(&gt;|z|)    \n(Intercept) 75764.96   87364.23    0.87    0.386    \nprice        8422.00    9959.54    0.85    0.398    \neprice      -7279.27    5735.91   -1.27    0.204    \noprice         81.31     492.19    0.17    0.869    \nlprice      -7525.80    8569.24   -0.88    0.380    \nheating        45.26       9.92    4.56    5e-06 ***\nincome          7.08       3.97    1.79    0.074 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    9.26e+10\nResidual Sum of Squares: 6.92e+10\nR-Squared:      0.252\nAdj. R-Squared: 0.211\nChisq: 36.4203 on 6 DF, p-value: 2.28e-06\n\n\nFrom the Random effects model, we may interpret the variables as such: For a 1 increase in the price of natural gas, as associate a 8422 unit increase in gas consumption. For a 1 unit increase in the price of electricity, we expect a 7279.27 unit decrease in gas consumption. For a 1 unit increase in the price of distillate fuel oil, we associate a 81.31 unit increase in gas consumption. For a 1 unit increase in the price of liquefied petroleum gas, we associate a 7525.8 unit decrease in gas consumption. For a 1 unit increase in the heating degree days, we associate a 45.26 unit increase in gas consumption. Finally, for a 1 unit icnrease in income, we associate a 7.08 unit increase in gas consumption.\n\n# Random Effects Test\nconsumptionReTest &lt;- plmtest(pooledreg1, effect=\"twoways\")\nconsumptionReTest\n\n\n    Lagrange Multiplier Test - two-ways effects (Honda)\n\ndata:  consumption ~ price + eprice + oprice + lprice + heating + income\nnormal = 9.9, p-value &lt;2e-16\nalternative hypothesis: significant effects\n\n\nThe LM test shows that the null hypothesis of zero variance in the two-way specifc errors (of indvidiuals and time) is rejected; therefore, heterogeneity among individuals and time may be significant. Therefore, we should not use the pooled model.\n\n# Compared fixed effects (two-ways) and REM\nphtest(withinreg3, random_effects)\n\n\n    Hausman Test\n\ndata:  consumption ~ price + eprice + oprice + lprice + heating + income\nchisq = 28, df = 6, p-value = 0.0001\nalternative hypothesis: one model is inconsistent\n\n\nFrom the Hausman Test, our p-value is well less than alpha = 0.05, and therefore we reject the null, which implies that one model is inconsistent and we implement the twoway fixed effect model as our preferred model. As we are to use the fixed effects, we would likely want to use the Hausman Taylor estimators, though we must ensure that we have enough instruments. The number of time-varying variables should be at least as much as the number of time-invariant variables.\nDiscuss the potential real-world implications of your findings or possible applications. (eg. How might it inform a policy decision or translate to earning potential;how would this be quantified?\n\nwithinreg3 &lt;- plm(consumption~price+eprice+oprice+lprice+heating+income, model=\"within\", effect = \"twoways\", data=natural_gas_pd)\nsummary(withinreg3)\n\nTwoways effects Within Model\n\nCall:\nplm(formula = consumption ~ price + eprice + oprice + lprice + \n    heating + income, data = natural_gas_pd, effect = \"twoways\", \n    model = \"within\")\n\nBalanced Panel: n = 5, T = 23, N = 115\n\nResiduals:\n   Min. 1st Qu.  Median 3rd Qu.    Max. \n -60442  -11674    -136   12153   56993 \n\nCoefficients:\n        Estimate Std. Error t-value Pr(&gt;|t|)    \nprice   12557.58   11579.83    1.08     0.28    \neprice  -8425.45    5926.83   -1.42     0.16    \noprice    803.70    1000.90    0.80     0.42    \nlprice  12545.43   12460.36    1.01     0.32    \nheating    60.55      11.03    5.49  4.4e-07 ***\nincome     -2.03       5.67   -0.36     0.72    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    6.12e+10\nResidual Sum of Squares: 4.16e+10\nR-Squared:      0.32\nAdj. R-Squared: 0.0544\nF-statistic: 6.42587 on 6 and 82 DF, p-value: 0.000014\n\n\nIn conclusion, from the analyses and tests that were run, we will implement the two-way fixed effect model as our preferred model. By controlling for both individual (state-specific) and time-specific unobserved heterogeneity, the model isolates the within-state, over-time variation in key variables such as energy prices, income, and weather (heating degree days). Heating degree days are highly significant, reaffirming the impact of climate on consumption. In contrast, income and prices show weaker within-state effects, suggesting that behavioral or structural consumption patterns are relatively stable in the short run. These findings could inform policy by emphasizing the limited short-run responsiveness of natural gas consumption to price signals. Instead structural changes such as updating energy efficiency standards or improving home insulation, may be more effective. Moreover, the large and significant role of heating degree days suggests that climate variability or change will continue to be a critical factor in energy planning and state-level resource allocation. A few limitations for our analyses include the small number of individual states as we only included 5 which limits the generalizability of the effects across states."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "personal website",
    "section": "",
    "text": "Economics 144 - US Employment Models\n\n\n\n\n\n\n\n\n\n\n\n\nEconomics 144 - AirPassengers\n\n\n\n\n\n\n\n\n\n\n\n\nEconomics 104 - Natural Gas Panel Dataset\n\n\n\n\n\n\n\n\n\n\n\n\nEconomics 104 - Swiss Labor Market Dataset\n\n\n\n\n\n\n\n\n\n\n\n\nEconomics 103 - Gym Members Exercise Tracking\n\n\n\n\n\nNo matching items"
  }
]