{
  "hash": "8a0346e61b847d6a2681335bc3efbc1f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Economics 144 - US Employment Models\"\nimage: ../us_employment.png\nauthor: \"Karen Tan, Prisha Narasimhan, Chris Bordbar\"\ndate: \"2025-12-06\"\n\nformat:\n  html: default\n  pdf:\n    pdf-engine: xelatex\ndescription: |\n  <br>\n  This project analyzes and forecasts US nonfarm employment using multiple time series models, including ARIMA, ETS, Holt-Winters, NNETAR, and Prophet. A combined forecast is also created to evaluate performance across models.\n  \n   **My Role:** Introduction, Prophet Model, Combined Forecast Model, Conclusion & Future Work\n---\n\n\n\n\nI. Introduction \n  \n  In this project, we analyze and forecast US employment using the \"us_employment\" dataset from the ffp3 package in RStudio. The dataset contains monthly employment data from January 1939 to June 2019. Our focus is on the series \"All Employees, Total Nonfarm,\" which captures total US employment excluding farmworkers.\n  We split the data into training and test sets to evaluate several forecasting methods and compare their predictions against the actual test data. Specifically, we apply ARIMA, ETS, Holt-Winters, NNETAR, and Prophet models, and also create a combined forecast to assess whether averaging multiple forecasts improves accuracy. Model performance is evaluated using accuracy metrics, residual diagnostics, and autocorrelation tests, allowing us to identify the model that best captures the patterns in total nonfarm employment and provides the most reliable forecasts.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntotal_nonfarm <- us_employment %>%\n  filter(Title == \"All Employees, Total Nonfarm\")\n```\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemployment_ts <- ts(total_nonfarm$Employed, start=1939, frequency=12)\nplot(employment_ts, xlab=\"Year\", ylab=\"Total Employment\", main=\"US Total Employment (Nonfarm)\")\n```\n\n::: {.cell-output-display}\n![](01-project_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nThe time series plot for total employment in the US (nonfarming) shows an increasing trend from 1939 to 2019. There are some small dips in the number of employed throughout the years, but it does look like the general total employment increases. Seasonality in the data is also visible, and especially so past 1960 where the fluctuations indicating seasonality become more regular and continue on until the last observations. Some cycles may also be present in the data as the rises and dips in employment come in groups every so often. For example, the cycling can be seen around 1950, 1980, 2000, and 2008.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntsdisplay(employment_ts)\n```\n\n::: {.cell-output-display}\n![](01-project_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nThe tsdisplay showcasing the ACF and PACF plot of the total employment data shows a very prolonged decay of the spikes in the ACF plot, suggesting high persistence because of how slow the spikes take to get down to a magnitude of 0. The PACF plot shows one very significant spike out of the bound windows, with 3 very slightly significant spikes further along the lags that could suggest some seasonal component could be taken into account for a model. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nemployment_ts.train <- window(employment_ts, end=c(2017,6))\nemployment_ts.test <- window(employment_ts, start=c(2017,7))\n\nautoplot(employment_ts) +\n  autolayer(employment_ts.train, series=\"Training\") +\n  autolayer(employment_ts.test, series=\"Test\") + theme_minimal()\n```\n\n::: {.cell-output-display}\n![](01-project_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nII. Results\n\n## ARIMA Forecast \n\n::: {.cell}\n\n```{.r .cell-code}\nemployment_arima <- auto.arima(employment_ts.train)\nemployment_arima\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSeries: employment_ts.train \nARIMA(2,0,1)(0,1,1)[12] with drift \n\nCoefficients:\n         ar1      ar2      ma1     sma1     drift\n      1.8946  -0.9000  -0.6119  -0.6396  123.1009\ns.e.  0.0210   0.0209   0.0376   0.0281   15.8391\n\nsigma^2 = 49198:  log likelihood = -6346.03\nAIC=12704.06   AICc=12704.15   BIC=12733.07\n```\n\n\n:::\n:::\n\n\nFor the ARIMA model for total (nonfarm) employment in the US, the auto.arima output suggests that an ARIMA(2,0,1)(0,1,1) process would fit the data. This result suggests an AR(2), MA(1) process with 1 seasonal differencing, and seasonal MA(1). It takes into account the increasing trend of the train set for employment, as well as the seasonality that is visible in the time series plot of the original data.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\narima_forecast <- forecast(employment_arima, h=27)\nautoplot(arima_forecast, xlim=c(2000,2020), ylim=c(120000,160000)) +\n  autolayer(employment_ts.test, series = \"Test\") + theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nScale for x is already present.\nAdding another scale for x, which will replace the existing scale.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 732 rows containing missing values or values outside the scale range\n(`geom_line()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](01-project_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nThe forecast using the ARIMA model fit looks to follow along well with the pattern and fluctuations of the actual observed data, especially in the first few steps ahead. However, after those first few steps, the ARIMA forecast does not seem to be able to keep up with the increasing trend that is visible in the observed data. The forecast does follow along with the general trend of increasing employment as the years increase, but is not able to match the magnitude of increase that has actually been recorded.  \n\n## ETS Forecast \n\n::: {.cell}\n\n```{.r .cell-code}\nemployment_ets <- ets(employment_ts.train)\n\nets_forecast <- forecast(employment_ets, h=27)\nautoplot(ets_forecast, xlim=c(2000,2020), ylim=c(120000,160000)) + \n  autolayer(employment_ts.test) + theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nScale for x is already present.\nAdding another scale for x, which will replace the existing scale.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 732 rows containing missing values or values outside the scale range\n(`geom_line()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](01-project_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nThe forecast for total US employment using the ETS method is similar to the ARIMA model where the general shape follows closely with the actual observed data. This method also has the same issue where it is unable to predict a higher magnitude of increase to the number of employed people in the US. The forecast here predicts lower employment numbers compared to the ARIMA model forecast. The observed data sits in the upper bounds of the confidence interval for this method. \n \n## Compare Training & Testing Errors\n\n::: {.cell}\n\n```{.r .cell-code}\nacc <- tibble(\n  model = c(\"ARIMA\", \"ETS\"),\n  RMSE  = c(\n    accuracy(arima_forecast, employment_ts.test)[\"Test set\", \"RMSE\"],\n    accuracy(ets_forecast,   employment_ts.test)[\"Test set\", \"RMSE\"]),\n  MAE = c(\n    accuracy(arima_forecast, employment_ts.test)[\"Test set\", \"MAE\"],\n    accuracy(ets_forecast,   employment_ts.test)[\"Test set\", \"MAE\"]),\n  MAPE = c(\n    accuracy(arima_forecast, employment_ts.test)[\"Test set\", \"MAPE\"],\n    accuracy(ets_forecast,   employment_ts.test)[\"Test set\", \"MAPE\"])\n)\nacc\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  model  RMSE   MAE  MAPE\n  <chr> <dbl> <dbl> <dbl>\n1 ARIMA 1118.  896. 0.596\n2 ETS   1778. 1507. 1.00 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemployment_tsibble <- as_tsibble(employment_ts)\n\n# Train/test split\ntrain <- employment_tsibble |> filter_index(~ \"2017-06\")\ntest  <- employment_tsibble |> filter_index(\"2017-07\" ~ .)\n\n# Fit models\nfit <- train |>\n  model(\n    arima   = ARIMA(value),\n    ets     = ETS(value)\n  )\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Forecast horizon\nh <- nrow(test)\nfc <- fit |> forecast(h = h)\n\n# Plot\nfc |> autoplot(employment_tsibble) +\n  coord_cartesian(xlim=c(yearmonth(\"2000 Jan\"), yearmonth(\"2020 Jan\")), ylim = c(120000, 160000))\n```\n\n::: {.cell-output-display}\n![](01-project_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Accuracy\nfc |> accuracy(test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 10\n  .model .type    ME  RMSE   MAE   MPE  MAPE  MASE RMSSE  ACF1\n  <chr>  <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 arima  Test   861. 1118.  896. 0.572 0.596   NaN   NaN 0.904\n2 ets    Test  1507. 1778. 1507. 1.00  1.00    NaN   NaN 0.892\n```\n\n\n:::\n:::\n\n\n## Holt-Winters, \n\n\n::: {.cell}\n\n```{.r .cell-code}\nh <- length(employment_ts.test)\n\nhw_add <- hw(employment_ts.train, seasonal = \"additive\", h = h)\nhw_mul <- hw(employment_ts.train, seasonal = \"multiplicative\", h = h)\n\nautoplot(employment_ts, series = \"Observed\") +\n  autolayer(hw_mul, series = \"HW Multiplicative\") +\n  autolayer(hw_add, series = \"HW Additive\") +\n  autolayer(employment_ts.test, series = \"Test (Actual)\") +\n  labs(\n    title = \"Holt–Winters Forecasts (Additive vs Multiplicative)\",\n    x = \"Year\",\n    y = \"Total Employment (Nonfarm)\"\n  ) +\n  guides(colour = guide_legend(title = \"Series\")) +\n  coord_cartesian(xlim = c(2000, 2020)) + theme_minimal()\n```\n\n::: {.cell-output-display}\n![](01-project_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n\n```{.r .cell-code}\nacc_hw_add <- accuracy(hw_add, employment_ts.test)[\"Test set\", c(\"RMSE\",\"MAE\",\"MAPE\")]\nacc_hw_mul <- accuracy(hw_mul, employment_ts.test)[\"Test set\", c(\"RMSE\",\"MAE\",\"MAPE\")]\n\nacc_summary <- tibble(\n  model = c(\"HW_additive\", \"HW_multiplicative\"),\n  RMSE = c(acc_hw_add[\"RMSE\"], acc_hw_mul[\"RMSE\"]),\n  MAE  = c(acc_hw_add[\"MAE\"],  acc_hw_mul[\"MAE\"]),\n  MAPE = c(acc_hw_add[\"MAPE\"], acc_hw_mul[\"MAPE\"])\n)\n\nacc_summary\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  model              RMSE   MAE  MAPE\n  <chr>             <dbl> <dbl> <dbl>\n1 HW_additive        259.  212. 0.141\n2 HW_multiplicative  526.  453. 0.302\n```\n\n\n:::\n:::\n\n\nThe additive model yields much lower RMSE, MAE, and MAPE (258.7, 211.6, 0.141) than the multiplicative model (525.8, 452.8, 0.302). Accordingly, we adopt the additive Holt–Winters specification for subsequent analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreferred_hw <- hw_add\nresid_hw <- residuals(preferred_hw)\n\nggtsdisplay(resid_hw, main = \"Residual Diagnostics — Holt–Winters Additive\")\n```\n\n::: {.cell-output-display}\n![](01-project_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n```{.r .cell-code}\nBox.test(resid_hw, lag = 24, type = \"Ljung-Box\", fitdf = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tBox-Ljung test\n\ndata:  resid_hw\nX-squared = 576.56, df = 24, p-value < 2.2e-16\n```\n\n\n:::\n:::\n\n\nThe residuals show clear serial correlation, with long runs of positive and negative errors and several persistent shocks. Additionally, the Ljung–Box test strongly rejects white noise (p < 2.2e-16), further confirming that substantial autocorrelation remains. Thus, while Holt–Winters captures the trend and seasonality, it does not fully model the underlying dynamics, and a more flexible model would be required for a proper statistical fit.\n\n## NNETAR\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n\nnnet_model <- nnetar(employment_ts.train)\n\nnnet_fc <- forecast(nnet_model, h = length(employment_ts.test))\n\nautoplot(nnet_fc) +\n  autolayer(employment_ts.test, series = \"Test\") +\n  labs(title = \"NNETAR Forecast\",\n       x = \"Year\",\n       y = \"Total Employment\") + theme_minimal()\n```\n\n::: {.cell-output-display}\n![](01-project_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n\n```{.r .cell-code}\nnnet_accuracy <- accuracy(nnet_fc, employment_ts.test)\nnnet_model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSeries: employment_ts.train \nModel:  NNAR(1,1,2)[12] \nCall:   nnetar(y = employment_ts.train)\n\nAverage of 20 networks, each of which is\na 2-2-1 network with 9 weights\noptions were - linear output units \n\nsigma^2 estimated as 607219\n```\n\n\n:::\n\n```{.r .cell-code}\nnnet_accuracy\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                       ME      RMSE       MAE         MPE      MAPE      MASE\nTraining set 2.232815e-02  779.2424  571.9611 -0.01166318 0.7053657 0.2770047\nTest set     2.698867e+03 3203.5652 2798.9652  1.79349778 1.8621064 1.3555581\n                  ACF1 Theil's U\nTraining set 0.2166367        NA\nTest set     0.7130423  2.921429\n```\n\n\n:::\n:::\n\n\nThe plot shows that the NNETAR model’s mean forecast becomes noticeably flattened and smooth, failing to reproduce the sharp swings that appear in the actual data. Additionally, the test-set errors are much larger than the training errors (RMSE = 3204, MAPE = 1.86%), showing that the model does not generalize well to data beyond the training set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntsdisplay(residuals(nnet_model), main = \"Residuals from NNETAR model\")\n```\n\n::: {.cell-output-display}\n![](01-project_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n\n```{.r .cell-code}\nBox.test(residuals(nnet_model), lag = 24, type = \"Ljung-Box\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tBox-Ljung test\n\ndata:  residuals(nnet_model)\nX-squared = 2069.1, df = 24, p-value < 2.2e-16\n```\n\n\n:::\n:::\n\n\nThe residuals from the NNETAR model exhibit noticeable remaining structure. In particular, the time-series plot shows recurring negative spikes consistent with seasonal patterns that the network did not fully capture. Additionally, the ACF and PACF plots display several statistically significant autocorrelations across a wide range of lags, indicating that the residuals are not behaving like white noise. Consistent with this, the Box–Ljung test strongly rejects the null of no autocorrelation (p < 2.2e−16). Together, these diagnostics suggest that the NNETAR model has not fully extracted all temporal dependence in the series, leaving meaningful autocorrelation in the residuals, which implies that the network would need a richer structure such as additional lag inputs, more hidden units, or a larger seasonal embedding to adequately learn the underlying dynamics of total nonfarm employment in the US.\n\n# Prophet\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Rcpp)\nlibrary(fable.prophet)\n\n# Prophet Model\nfit_prophet <- train |>\n  model(prophet(value ~ \n                  season(period = \"year\", order = 10) + \n                  season(period = \"month\", order = 3))) \n\nfit_prophet |>\n  components() |>\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](01-project_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Prophet Forecast\nfc_prophet <- fit_prophet |>\n  forecast(h = nrow(test))\n\nfc_prophet |>\n  autoplot(train) +\n    labs(x = \"Year\", y = \"Total Employment\",\n      title = \"US Nonfarm Employment Prophet Forecast\") + theme_minimal() + \n  autolayer(test, series = \"Test\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nPlot variable not specified, automatically selected `.vars = value`\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in geom_line(eval_tidy(expr(aes(!!!aes_spec))), data = object, ..., :\nIgnoring unknown parameters: `series`\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](01-project_files/figure-html/unnamed-chunk-17-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# Accuracy on test data\nprophet_accuracy_test <- accuracy(fc_prophet, test)\n\n# Accuracy on training data\nprophet_accuracy_train <- accuracy(fit_prophet)\n\nprophet_accuracy <- dplyr::bind_rows(\n  prophet_accuracy_train |> mutate(data = \"Training\"),\n  prophet_accuracy_test  |> mutate(data = \"Test\"))\n\nprophet_accuracy\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 11\n  .model     .type      ME  RMSE   MAE     MPE  MAPE    MASE   RMSSE  ACF1 data \n  <chr>      <chr>   <dbl> <dbl> <dbl>   <dbl> <dbl>   <dbl>   <dbl> <dbl> <chr>\n1 \"prophet(… Trai… 9.69e-2 1949. 1460. -0.0640  1.81   0.707   0.820 0.975 Trai…\n2 \"prophet(… Test  7.86e+3 7967. 7863.  5.26    5.26 NaN     NaN     0.853 Test \n```\n\n\n:::\n:::\n\n\nThe Prophet model captures the seasonal patterns in US nonfarm employment and shows the general upward trend. However, the forecast underestimates the magnitude of increases in the test period, resulting in poor accuracy (test RMSE = 7977.4, MAE = 7874.5). While training performance is reasonable, the model struggles to forecast the recent trend, making it less reliable than Holt-Winters or ARIMA for this dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_prophet |> gg_tsresiduals()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: `gg_tsresiduals()` was deprecated in feasts 0.4.2.\nℹ Please use `ggtime::gg_tsresiduals()` instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](01-project_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n\n```{.r .cell-code}\nprophet_resid <- residuals(fit_prophet)\nprophet_resid_vec <- as.numeric(prophet_resid$.resid)\nBox.test(prophet_resid_vec, lag = 24, type = \"Ljung-Box\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tBox-Ljung test\n\ndata:  prophet_resid_vec\nX-squared = 8385, df = 24, p-value < 2.2e-16\n```\n\n\n:::\n:::\n\n\nThe residuals of the Prophet model generally fluctuate around zero, but the variance is clearly non-constant, with periods of larger and smaller deviations. The Ljung-Box test strongly rejects the null hypothesis of white noise (p < 2.2e-16) confirming that the residuals are highly autocorrelated and that the model fails to capture all temporal structure in the data. Overall, this suggests that while Prophet captures the broad trend and seasonality, it leaves substantial patterns unexplained, particularly in short-term dynamics. \n\n# Combination Forecast (ARIMA, ETS, Holt-Winters, NN)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load libraries\nlibrary(forecast)\nlibrary(fable)\nlibrary(fable.prophet)\nlibrary(tsibble)\nlibrary(ggplot2)\nlibrary(dplyr)\n\nemployment_ts.train <- window(employment_ts, end=c(2017,6))\nemployment_ts.test  <- window(employment_ts, start=c(2017,7))\nh <- length(employment_ts.test)\n\nemployment_tsibble <- as_tsibble(employment_ts.train)\n\n# ARIMA\nemployment_arima <- auto.arima(employment_ts.train)\narima_forecast <- forecast(employment_arima, h=h)\n\n# ETS\nemployment_ets <- ets(employment_ts.train)\nets_forecast <- forecast(employment_ets, h=h)\n\n# Holt-Winters (additive)\nhw_forecast <- hw(employment_ts.train, seasonal = \"additive\", h=h)\n\n# NNETAR\nnnet_model <- nnetar(employment_ts.train)\nnnet_fc <- forecast(nnet_model, h=h)\n\n# Prophet\nfit_prophet <- employment_tsibble |>\n  model(\n    prophet(value ~ season(period = \"year\", order = 10) + \n              season(period = \"month\", order = 3)))\nfc_prophet <- fit_prophet |> forecast(h = h)\n\n# Prophet mean as forecast\nprophet_ts <- ts(as.numeric(fc_prophet$.mean),\n                 start=start(employment_ts.test),\n                 frequency=12)\n\n# Combined Forecast\nfc_combo <- (arima_forecast$mean + ets_forecast$mean + hw_forecast$mean +\n                nnet_fc$mean + prophet_ts) / 5\n\ncombo_ts <- ts(fc_combo, start=start(employment_ts.test), frequency=12)\n\n# Plot all forecasts and combination\nautoplot(window(employment_ts, start=c(2000,1))) +\n  autolayer(arima_forecast, series=\"ARIMA\", PI = FALSE) +\n  autolayer(ets_forecast, series=\"ETS\", PI = FALSE) +\n  autolayer(hw_forecast, series=\"Holt-Winters\", PI = FALSE) +\n  autolayer(nnet_fc, series=\"NNETAR\", PI = FALSE) +\n  autolayer(prophet_ts, series=\"Prophet\") +\n  autolayer(combo_ts, series=\"Combination\") +\n  xlab(\"Year\") + ylab(\"Total Employment\") +\n  ggtitle(\"Forecast Combination for US Nonfarm Employment\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](01-project_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\nAs seen in the above plot, the Holt-Winters model appears to track the actual test data most closely, capturing much of the seasonality, trend, and fluctuations. The ARIMA and ETS forecasts appear to produce similar predictions, following the seasonality of the test data, but seemingly underestimates the magnitude of the trend a bit. The NNETAR forecast is rather unique, in that it appears to be generally slightly increasing, but does not capture the seasonality or strength of the trend. The Prophet forecast demonstrates the seasonality in the test data, but fails to forecast at an appropriate height and underestimates the increasing trend, resulting in predictions that are perhaps the most far off from the test data. The combination forecast provides a middle-ground prediction that balances the strengths and weaknesses of the individual models (though the prophet forecast has a significant effect on the height), producing perhaps a more stable forecast than any single model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Accuracy on the test data\nacc_arima   <- accuracy(arima_forecast$mean, employment_ts.test)\nacc_ets     <- accuracy(ets_forecast$mean, employment_ts.test)\nacc_hw      <- accuracy(hw_forecast$mean, employment_ts.test)\nacc_nnet    <- accuracy(nnet_fc$mean, employment_ts.test)\nacc_prophet <- accuracy(prophet_ts, employment_ts.test)\nacc_combo   <- accuracy(combo_ts, employment_ts.test)\n\n# Combine all results in one table\naccuracy_table <- rbind(\n  data.frame(Model=\"ARIMA\", acc_arima),\n  data.frame(Model=\"ETS\", acc_ets),\n  data.frame(Model=\"Holt-Winters\", acc_hw),\n  data.frame(Model=\"NNETAR\", acc_nnet),\n  data.frame(Model=\"Prophet\", acc_prophet),\n  data.frame(Model=\"Combination\", acc_combo)\n)\n\naccuracy_table\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 Model        ME      RMSE       MAE       MPE      MAPE\nTest set         ARIMA  861.1017 1117.8595  896.2429 0.5717054 0.5956312\nTest set1          ETS 1506.9323 1778.2519 1506.9323 1.0028195 1.0028195\nTest set2 Holt-Winters  183.0432  258.7163  211.6124 0.1219424 0.1413711\nTest set3       NNETAR 2697.8265 3212.9025 2802.6637 1.7926404 1.8644962\nTest set4      Prophet 7875.8166 7979.6997 7875.8166 5.2651452 5.2651452\nTest set5  Combination 2624.9440 2782.8398 2624.9440 1.7508506 1.7508506\n               ACF1 Theil.s.U\nTest set  0.9043343 1.0186841\nTest set1 0.8921786 1.6217105\nTest set2 0.5862243 0.2367557\nTest set3 0.7154183 2.9295736\nTest set4 0.8543141 7.2468189\nTest set5 0.8402855 2.5382386\n```\n\n\n:::\n:::\n\n\nThe results from the accuracy function overall seem to confirm the observations of the plot. The forecast evaluation on US (nonfarm) employment show that the additive Holt-Winters model preferred best, achieving the lowest errors (RMSE = 258.7, MAE = 211.6, MAPE = 0.14) and low residual autocorrelation (ACF1 = 0.58), indicating a strong fit to the test data. Therefore, this is our preferred model. ARIMA was a reasonable alternative, while ETS, NNETAR, and Prophet exhibited higher errors and more correlated residuals, with Prophet performing particularly poorly. The combination of all the models did not improve accuracy due to the inclusion of forecasts that performed poorly compared to the Holt-Winters model. \n\nIII. Conclusion and Future Work\n\n  The analysis of US nonfarm employment from 1939 to 2019 shows a clear upward trend, regular seasonality, and periodic cycles. Among the models tested, the additive Holt-Winters method provided the most accurate forecasts, effectively capturing trend, seasonality, and fluctuations. ARIMA and ETS performed reasonably well but slightly underestimated recent growth, while NNETAR and Prophet struggled to reproduce sharp swings and trends, leading to larger forecast errors. The combination forecast provided a stable middle-ground prediction, though it seemed to be largely influenced by the weaker models. \n  Future work may explore weighted combination forecasts, or using more advanced neural network models to better capture nonlinear patterns. Additionally, Markov Chain approaches could be investigated to model employment  changes, capturing structural shifts in the labor market that standard time series models may miss. Incorporating external economic indicators or finding structural breaks may further improve forecast accuracy as well as allow us to better understand the contributors behind total employment, making predictions of US nonfarm employment more reliable for decision-making.\n\nIV. References\n\nThe dataset is originally from the US Bureau of Labor Statistics. However we have included the citation for the ffp3 package as well. \n\nHyndman, R. J., & Athanasopoulos, G. (2025). us_employment: Monthly U.S. Employment Data (1939–2019). In fpp3 (R package). Retrieved from CRAN. Original data from the U.S. Bureau of Labor Statistics.\n\nU.S. Bureau of Labor Statistics. Current Employment Statistics (National), “All Employees, Total Nonfarm Payroll Employment” (Seasonally Adjusted), Series ID CES3000000001. Retrieved via fpp3 package in R.\n\n",
    "supporting": [
      "01-project_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}